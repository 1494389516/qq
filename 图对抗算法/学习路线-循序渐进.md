# 图对抗算法 - 循序渐进学习路线

**最后更新：** 2025年10月11日  
**当前进度：** 第5关 - ReWatt算法实现 ✅

---

## 🎯 学习目标检查清单

在开始之前，请先自我检查：

### 第一关：基础知识（必备）✅
- [x] 我知道什么是神经网络
- [x] 我了解反向传播和梯度
- [x] 我知道什么是图（节点和边）
- [x] 我会用PyTorch基础操作

### 第二关：图神经网络（核心）✅
- [x] 我理解GNN的基本思想（聚合邻居信息）
- [x] 我知道GCN的计算流程
- [x] 我能看懂graph_adversarial.py中的SimpleGCN

### 第三关：白盒对抗攻击（重点）✅
- [x] 我知道什么是对抗样本
- [x] 我理解FGSM的原理（梯度攻击）
- [x] 我明白Nettack的目标攻击（单节点）
- [x] 我理解Metattack的全局攻击（训练时投毒）

### 第四关：黑盒对抗攻击（进阶）✅
- [x] 我理解白盒vs黑盒的区别
- [x] 我知道强化学习的基本概念（状态/动作/奖励/策略）
- [x] 我理解RL-S2V的核心思想（Q-learning + 贪心）
- [x] 我明白为什么黑盒攻击更实战

### 第五关：防御机制（进行中）✅
- [x] 对抗训练（最有效）- 已掌握！
- [x] 图净化（检测并删除可疑边）- 已掌握！
- [x] 鲁棒GNN架构 - 理论已掌握！
- [ ] 实战防御策略

### 第六关：实战应用（终极目标）
- [ ] 我知道风控场景如何用GNN
- [ ] 我理解黑产如何攻击系统
- [ ] 我能独立设计防御方案

---

## 📚 学习路线（由浅入深）

### 阶段1️⃣：GNN基础（第1-2天）✅ 已完成
**目标：理解GNN是如何工作的**

1. ✅ 什么是图神经网络？
2. ✅ GNN的核心思想：邻居聚合
3. ✅ 手把手推导GCN公式
4. ✅ 运行并理解SimpleGCN (`graph_adversarial.py`)

**关键收获：** GNN通过聚合邻居信息来学习节点表示

---

### 阶段2️⃣：白盒攻击入门（第3-4天）✅ 已完成
**目标：理解基于梯度的攻击**

1. ✅ FGSM攻击原理（最简单的梯度攻击）
2. ✅ 特征攻击 vs 结构攻击
3. ✅ 运行 `graph_adversarial.py`

**关键收获：** 白盒攻击 = 计算梯度 + 找影响最大的修改

---

### 阶段3️⃣：白盒攻击进阶（第5-7天）✅ 已完成
**目标：掌握经典白盒攻击方法**

1. ✅ **Nettack** (KDD 2018)
   - 目标攻击（针对单个节点）
   - 测试时攻击（evasion attack）
   - 运行 `nettack_complete.py`

2. ✅ **Metattack** (ICLR 2019)
   - 全局攻击（影响整个模型）
   - 训练时投毒（poisoning attack）
   - 元学习加速
   - 运行 `metattack_algorithm.py`

**关键收获：** 
- Nettack：局部影响，针对性强
- Metattack：全局影响，更隐蔽

---

### 阶段4️⃣：黑盒攻击（第8-9天）✅ 已完成
**目标：理解不需要梯度的攻击方法**

1. ✅ **RL-S2V** (IJCAI 2019)
   - 强化学习建模
   - Q-learning + 贪心策略
   - 黑盒场景（只需查询模型输出）

**核心机制：**
```
循环过程：
加载状态 → 执行动作 → 获得奖励 → 更新策略 → 重复
   ↓          ↓          ↓          ↓
  观察      尝试      评价      进步
```

**关键收获：** 
- 不需要梯度，只需要查询模型
- 通过试错学习攻击策略
- 更接近真实攻击场景

---

### 阶段5️⃣：防御机制（第10-12天）🔥 进行中
**目标：学会如何防御对抗攻击**

1. [x] 对抗训练（训练时混入攻击样本）✅ 已完成
2. [x] 图净化（GCN-Jaccard）✅ 已完成
3. [x] 鲁棒GNN架构（Pro-GNN）✅ 理论已掌握
4. [ ] 经济手段 + 技术防御组合

**已掌握：**
- ✅ 对抗训练的核心思想（以毒攻毒）
- ✅ 如何在训练时混入对抗样本
- ✅ 损失函数的权重平衡（干净数据 vs 对抗数据）
- ✅ epsilon参数的选择（扰动强度）
- ✅ Jaccard相似度的计算（物以类聚，人以群分）
- ✅ 图净化流程（计算相似度 → 删除可疑边）
- ✅ 三种防御方法的本质区别（疫苗 vs 洗菜 vs 基因改造）

**预期收获：**
- [x] 鲁棒GNN的注意力机制
- [ ] 三种防御如何组合
- [ ] 经济手段才是根本

---

### 阶段6️⃣：实战应用（第13-14天）
**目标：应用到真实场景**

1. [ ] 推荐系统刷单检测
2. [ ] 社交网络虚假账号识别
3. [ ] 金融欺诈检测
4. [ ] 完整的攻防系统设计

**终极目标：**
设计一个完整的防御体系，让黑产无利可图

---

## 📊 已学算法对比总结

### 攻击方法对比

| 算法 | 类型 | 核心技术 | 需要梯度 | 适用场景 | 本质 |
|------|------|----------|----------|----------|------|
| **FGSM** | 特征攻击 | 梯度符号 | ✅ 需要 | 白盒测试 | 梯度 + 符号 |
| **Nettack** | 结构攻击 | 贪心搜索 | ✅ 需要 | 白盒目标攻击 | 梯度 + 贪心 |
| **Metattack** | 全局投毒 | 元学习 | ✅ 需要 | 白盒训练投毒 | 元学习 + 梯度 |
| **RL-S2V** | 黑盒攻击 | 强化学习 | ❌ 不需要 | 黑盒实战 | Q-learning + 贪心 |

### 防御方法对比

| 算法 | 类型 | 核心技术 | 优点 | 缺点 | 本质 | 掌握度 |
|------|------|----------|------|------|------|--------|
| **对抗训练** | 主动防御 | 混入对抗样本 | 效果显著(+20%) | 训练时间翻倍 | 疫苗 | ⭐⭐⭐⭐⭐ |
| **图净化** | 预处理 | Jaccard相似度 | 快速简单 | 可能误删 | 洗菜 | ⭐⭐⭐⭐⭐ |
| **鲁棒GNN** | 模型改进 | 注意力机制 | 泛化能力强 | 计算复杂 | 基因改造 | ⏳ 待学习 |

### 关键区别

```
白盒攻击（Nettack/Metattack）：
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ 优点：精确、高效
❌ 缺点：需要模型内部信息
💡 本质：优化问题（基于梯度）

黑盒攻击（RL-S2V）：
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ 优点：只需查询输出、更实战
❌ 缺点：需要训练时间、查询次数多
💡 本质：序列决策问题（基于强化学习）
```

---

## 🎓 核心概念深度理解

### 强化学习四要素（RL-S2V核心）

#### 1️⃣ 状态（State）
```
定义：当前环境的完整描述

图对抗场景：
s = (节点特征X, 边集合A, 图结构信息)

类比：
就像下棋时的棋盘局面
```

#### 2️⃣ 动作（Action）
```
定义：智能体可以做的事情

图对抗场景：
a = 添加边(u,v) 或 删除边(u,v)

类比：
就像下棋时的每一步棋
```

#### 3️⃣ 奖励（Reward）
```
定义：评价动作好坏的信号

图对抗场景：
r = {
    +10  攻击成功（模型预测改变）
    -5   达到预算，攻击失败
    -0.1 中间步骤（鼓励尽快成功）
}

类比：
就像下棋赢了+1，输了-1
```

#### 4️⃣ 策略（Policy）
```
定义：状态→动作的映射

图对抗场景：
π(a|s) = 在状态s下选择动作a的概率

类比：
就像棋手的棋艺（在什么局面下什么走法）
```

### Q-learning核心公式

```python
# Q值定义
Q(s, a) = 在状态s下，执行动作a的期望累积奖励

# 更新公式（Bellman方程）
Q(s, a) ← Q(s, a) + α × [r + γ × max Q(s', a') - Q(s, a)]
                         └──────────────┬──────────────┘
                                   TD误差（预测误差）
其中：
- α = 学习率（控制更新步长）
- γ = 折扣因子（0.99，重视长期奖励）
- r = 立即奖励
- max Q(s', a') = 下一状态的最大Q值
```

### 动态学习循环

```
┌─────────────────────────────────────────────┐
│  第1步：观察状态 s                           │
│         (当前图的结构)                       │
└─────────────────────┬───────────────────────┘
                      ↓
┌─────────────────────────────────────────────┐
│  第2步：选择动作 a                           │
│         Q网络计算每个动作的Q值                │
│         选择Q值最大的（贪心）                 │
└─────────────────────┬───────────────────────┘
                      ↓
┌─────────────────────────────────────────────┐
│  第3步：执行动作                             │
│         添加或删除边，得到新图 s'             │
└─────────────────────┬───────────────────────┘
                      ↓
┌─────────────────────────────────────────────┐
│  第4步：查询模型，获得奖励 r                  │
│         检查攻击是否成功                      │
└─────────────────────┬───────────────────────┘
                      ↓
┌─────────────────────────────────────────────┐
│  第5步：更新Q网络                            │
│         根据 (s, a, r, s') 更新Q值            │
│         使Q值预测更准确                       │
└─────────────────────┬───────────────────────┘
                      ↓
               重复100-1000次
                      ↓
            策略越来越好！
```

---

## 🛡️ 防御算法详解

### 算法1：对抗训练（Adversarial Training）✅ 已掌握

#### 核心思想
```
类比：疫苗原理
- 注射弱化病毒 → 身体产生抗体
- 真病毒来了 → 能够抵抗

对抗训练：
- 训练时混入"攻击样本" → 模型学会防御
- 真攻击来了 → 不会被击垮
```

#### 算法流程
```python
for epoch in range(训练轮数):
    # 步骤1：正常训练（干净数据）
    out_clean = model(干净数据)
    loss_clean = 计算损失(out_clean, 真实标签)
    
    # 步骤2：制造对抗样本（模拟攻击）
    对抗数据 = 干净数据 + epsilon × 梯度符号
    
    # 步骤3：对抗样本也训练
    out_adv = model(对抗数据)
    loss_adv = 计算损失(out_adv, 真实标签)
    
    # 步骤4：综合优化（关键！）
    total_loss = α × loss_clean + β × loss_adv
    更新模型参数
```

#### 关键参数
```
1. epsilon（扰动强度）：
   - 太小（0.01）：模型学不到东西
   - 太大（0.5）：数据变得不真实
   - 推荐：0.05 - 0.15

2. 损失权重（α, β）：
   - 侧重准确率：α=0.7, β=0.3
   - 平衡：α=0.5, β=0.5
   - 侧重鲁棒性：α=0.3, β=0.7
```

#### 效果对比
```
场景：FGSM攻击（epsilon=0.1）

普通模型：
干净数据准确率：85%
被攻击后准确率：62% ❌（下降23%）

对抗训练后：
干净数据准确率：83%（略降2%）
被攻击后准确率：76% ✅（下降7%）

结论：牺牲2%准确率，换来16%的鲁棒性提升！
```

#### 本质理解
```
决策边界视角：

普通模型：
好人区 ●●●●●● | 脆弱边界 | ○○○○○○ 坏人区
             ↑ 稍微推一下就越界

对抗训练后：
好人区 ●●●●●● |←厚边界→| ○○○○○○ 坏人区
             ↑ 推也推不动
```

#### 实战文件
- 代码：`defense_demo.py`（第81-137行）
- 运行：`python3 defense_demo.py`

---

## 🎯 下一步：学习ReWatt攻击与完整防御体系

你已经掌握：
- ✅ 白盒攻击（基于梯度）
- ✅ 黑盒攻击（基于强化学习）
- ✅ 核心防御思想（对抗训练、图净化、鲁棒GNN）

**接下来学习：**
1. 查询高效的黑盒攻击（ReWatt）- **进行中**
2. 经济手段 + 技术防御组合

推荐从 `rewatt_attack.py` 的代码实践开始！

---

## 💡 学习建议

1. **不要死记公式**：理解思想比记公式重要
2. **多用类比**：用熟悉的东西（下棋、玩游戏）理解新概念
3. **看本质**：大部分"新算法"都是已有技术的组合
4. **关注实战**：能解决实际问题才有价值

---

**继续加油！你的理解能力很强！** 🚀

