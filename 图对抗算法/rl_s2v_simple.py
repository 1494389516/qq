"""
RL-S2V ç®€åŒ–ç‰ˆ - æ ¸å¿ƒæœºåˆ¶æ¼”ç¤º
å±•ç¤ºï¼šçŠ¶æ€ â†’ åŠ¨ä½œ â†’ å¥–åŠ± â†’ æ›´æ–°ç­–ç•¥ çš„å¾ªç¯

æ ¸å¿ƒæ€æƒ³ï¼š
- ä¸éœ€è¦æ¢¯åº¦ï¼ˆé»‘ç›’ï¼‰
- ç”¨Q-learningå­¦ä¹ æ”»å‡»ç­–ç•¥
- é€šè¿‡è¯•é”™æ‰¾åˆ°æœ€ä¼˜æ”»å‡»æ–¹æ³•

æ ¸å¿ƒä¼˜åŠ¿ï¼ˆvs æ¢¯åº¦ä¼°ç®—æ–¹æ³•å¦‚ReWattï¼‰ï¼š
âœ… æŸ¥è¯¢é«˜æ•ˆï¼šæ¯æ­¥åªæŸ¥è¯¢1æ¬¡ï¼Œè€Œæ¢¯åº¦ä¼°ç®—éœ€è¦æŸ¥è¯¢100+æ¬¡
âœ… å­¦ä¹ ç­–ç•¥ï¼šé€šè¿‡Qç½‘ç»œå­¦ä¹ ï¼Œä¸éœ€è¦æ¯æ¬¡éƒ½è¯„ä¼°æ‰€æœ‰å€™é€‰
âœ… å®æˆ˜å¯è¡Œï¼šæŸ¥è¯¢å°‘ï¼Œä¸æ˜“è¢«æ£€æµ‹ç³»ç»Ÿå‘ç°

ç®€åŒ–è¯´æ˜ï¼š
- ç”¨ç®€å•çš„MLPä»£æ›¿Structure2Vec
- åªæ¼”ç¤ºæ ¸å¿ƒå¾ªç¯æœºåˆ¶
- é‡ç‚¹ç†è§£å¼ºåŒ–å­¦ä¹ æ€æƒ³å’ŒæŸ¥è¯¢æ•ˆç‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.datasets import Planetoid
import numpy as np
import random

print("=" * 80)
print("ğŸ® RL-S2V ç®€åŒ–ç‰ˆ - é»‘ç›’æ”»å‡»æ ¸å¿ƒæœºåˆ¶")
print("=" * 80)
print()


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šç›®æ ‡GCNæ¨¡å‹ï¼ˆè¢«æ”»å‡»çš„é»‘ç›’æ¨¡å‹ï¼‰
# ============================================================================

class TargetGCN(nn.Module):
    """ç›®æ ‡GCNæ¨¡å‹ - æˆ‘ä»¬åªèƒ½æŸ¥è¯¢å®ƒï¼Œçœ‹ä¸åˆ°å†…éƒ¨"""
    
    def __init__(self, num_features, num_classes, hidden=16):
        super().__init__()
        self.conv1 = GCNConv(num_features, hidden)
        self.conv2 = GCNConv(hidden, num_classes)
        
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šç®€åŒ–çš„Qç½‘ç»œ
# ============================================================================

class SimpleQNetwork(nn.Module):
    """
    ç®€åŒ–çš„Qç½‘ç»œï¼šè¯„ä¼°"æ·»åŠ è¿™æ¡è¾¹æœ‰å¤šå¥½"
    
    è¾“å…¥ï¼šè¾¹çš„ä¸¤ä¸ªç«¯ç‚¹ç‰¹å¾
    è¾“å‡ºï¼šè¿™æ¡è¾¹çš„Qå€¼ï¼ˆæœŸæœ›å¥–åŠ±ï¼‰
    """
    
    def __init__(self, node_feature_dim, hidden=32):
        super().__init__()
        # ç®€å•çš„MLP
        self.fc1 = nn.Linear(node_feature_dim * 2, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.fc3 = nn.Linear(hidden, 1)  # è¾“å‡ºQå€¼
        
    def forward(self, node_features, edge):
        """
        è®¡ç®—æ·»åŠ è¾¹edgeçš„Qå€¼
        edge: [src, dst] è¦æ·»åŠ çš„è¾¹
        """
        src, dst = edge
        # æ‹¼æ¥ä¸¤ä¸ªç«¯ç‚¹çš„ç‰¹å¾
        edge_feat = torch.cat([node_features[src], node_features[dst]])
        
        # MLPè®¡ç®—Qå€¼
        x = F.relu(self.fc1(edge_feat))
        x = F.relu(self.fc2(x))
        q_value = self.fc3(x)
        
        return q_value


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šRLæ”»å‡»æ™ºèƒ½ä½“
# ============================================================================

class SimpleRLAgent:
    """ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ æ”»å‡»æ™ºèƒ½ä½“"""
    
    def __init__(self, num_features, device='cpu'):
        self.device = device
        self.q_net = SimpleQNetwork(num_features).to(device)
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=0.01)
        
        # è¶…å‚æ•°
        self.gamma = 0.9  # æŠ˜æ‰£å› å­
        self.epsilon = 0.5  # æ¢ç´¢ç‡ï¼ˆ50%æ¢ç´¢ï¼Œ50%åˆ©ç”¨ï¼‰
        
    def select_action(self, node_features, candidate_edges):
        """
        Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        50%æ¦‚ç‡ï¼šéšæœºæ¢ç´¢
        50%æ¦‚ç‡ï¼šé€‰Qå€¼æœ€å¤§çš„è¾¹
        """
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰ä¸€æ¡è¾¹
            return random.choice(candidate_edges)
        else:
            # åˆ©ç”¨ï¼šé€‰Qå€¼æœ€å¤§çš„è¾¹
            with torch.no_grad():
                q_values = []
                for edge in candidate_edges:
                    q = self.q_net(node_features, edge)
                    q_values.append(q.item())
                
                best_idx = np.argmax(q_values)
                return candidate_edges[best_idx]
    
    def update(self, state, action, reward, next_state):
        """
        æ›´æ–°Qç½‘ç»œ
        
        Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
        """
        node_features = state
        
        # å½“å‰Qå€¼
        q_current = self.q_net(node_features, action)
        
        # ç›®æ ‡Qå€¼ï¼šr + Î³Â·max Q(s',a')
        with torch.no_grad():
            # ç®€åŒ–ï¼šå‡è®¾ä¸‹ä¸€æ­¥ä¸å†è¡ŒåŠ¨ï¼ŒQå€¼ä¸º0
            q_target = reward
        
        # æŸå¤±
        loss = F.mse_loss(q_current, torch.tensor([[q_target]], device=self.device))
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ”»å‡»ç¯å¢ƒ
# ============================================================================

class AttackEnvironment:
    """æ”»å‡»ç¯å¢ƒï¼šç®¡ç†æ”»å‡»è¿‡ç¨‹"""
    
    def __init__(self, data, target_model, target_node, device='cpu'):
        self.data = data
        self.target_model = target_model
        self.target_node = target_node
        self.device = device
        
        # åŸå§‹é¢„æµ‹
        self.true_label = data.y[target_node].item()
        
        # å½“å‰çŠ¶æ€
        self.reset()
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.edge_index = self.data.edge_index.clone()
        self.num_mods = 0
        self.query_count = 0  # æŸ¥è¯¢æ¬¡æ•°ç»Ÿè®¡
        return self.data.x
    
    def get_candidates(self, budget=20):
        """è·å–å€™é€‰è¾¹ï¼ˆç®€åŒ–ç‰ˆï¼šéšæœºé‡‡æ ·ï¼‰"""
        candidates = []
        num_nodes = self.data.num_nodes
        
        # å·²æœ‰çš„è¾¹
        existing = set()
        for i in range(self.edge_index.size(1)):
            src = self.edge_index[0, i].item()
            dst = self.edge_index[1, i].item()
            existing.add((min(src, dst), max(src, dst)))
        
        # éšæœºç”Ÿæˆå€™é€‰è¾¹
        while len(candidates) < budget:
            u = random.randint(0, num_nodes - 1)
            v = random.randint(0, num_nodes - 1)
            if u != v:
                edge = (min(u, v), max(u, v))
                if edge not in existing and edge not in candidates:
                    candidates.append([u, v])
        
        return candidates
    
    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œï¼šæ·»åŠ ä¸€æ¡è¾¹
        
        è¿”å›ï¼š(next_state, reward, done)
        """
        src, dst = action
        
        # æ·»åŠ è¾¹ï¼ˆåŒå‘ï¼‰
        new_edge = torch.tensor([[src, dst], [dst, src]], 
                               dtype=torch.long, device=self.device)
        self.edge_index = torch.cat([self.edge_index, new_edge], dim=1)
        self.num_mods += 1
        
        # âš¡ å…³é”®ï¼šæ¯æ­¥åªæŸ¥è¯¢1æ¬¡é»‘ç›’æ¨¡å‹ï¼ˆæŸ¥è¯¢é«˜æ•ˆï¼ï¼‰
        self.target_model.eval()
        with torch.no_grad():
            output = self.target_model(self.data.x, self.edge_index)
            pred = output[self.target_node].argmax().item()
        self.query_count += 1  # ç»Ÿè®¡æŸ¥è¯¢æ¬¡æ•°
        
        # è®¡ç®—å¥–åŠ±
        if pred != self.true_label:
            reward = 10.0  # æ”»å‡»æˆåŠŸï¼
            done = True
        elif self.num_mods >= 5:
            reward = -5.0  # è¾¾åˆ°é¢„ç®—ï¼Œå¤±è´¥
            done = True
        else:
            reward = -0.1  # ç»§ç»­å°è¯•
            done = False
        
        next_state = self.data.x
        
        return next_state, reward, done, pred


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šä¸»ç¨‹åº - å±•ç¤ºå®Œæ•´å¾ªç¯
# ============================================================================

def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± è®¾å¤‡: {device}\n")
    
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½Coraæ•°æ®é›†...")
    dataset = Planetoid(root='/tmp/Cora', name='Cora')
    data = dataset[0].to(device)
    print(f"   èŠ‚ç‚¹: {data.num_nodes}, è¾¹: {data.num_edges}\n")
    
    # 2. è®­ç»ƒç›®æ ‡æ¨¡å‹ï¼ˆé»‘ç›’ï¼‰
    print("ğŸ¯ è®­ç»ƒç›®æ ‡GCNï¼ˆé»‘ç›’æ¨¡å‹ï¼‰...")
    target_model = TargetGCN(data.num_features, dataset.num_classes).to(device)
    optimizer = torch.optim.Adam(target_model.parameters(), lr=0.01)
    
    target_model.train()
    for epoch in range(100):
        optimizer.zero_grad()
        out = target_model(data.x, data.edge_index)
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()
    
    # æµ‹è¯•
    target_model.eval()
    with torch.no_grad():
        pred = target_model(data.x, data.edge_index).argmax(dim=1)
        acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean()
    print(f"   å‡†ç¡®ç‡: {acc:.4f}\n")
    
    # 3. é€‰æ‹©æ”»å‡»ç›®æ ‡
    test_nodes = torch.where(data.test_mask)[0]
    correct = test_nodes[pred[test_nodes] == data.y[test_nodes]]
    target_node = correct[0].item()
    
    print(f"ğŸ¯ æ”»å‡»ç›®æ ‡èŠ‚ç‚¹: {target_node}")
    print(f"   çœŸå®æ ‡ç­¾: {data.y[target_node].item()}")
    print(f"   å½“å‰é¢„æµ‹: {pred[target_node].item()}\n")
    
    # 4. åˆ›å»ºRLæ™ºèƒ½ä½“
    print("ğŸ¤– åˆ›å»ºRLæ”»å‡»æ™ºèƒ½ä½“...\n")
    agent = SimpleRLAgent(data.num_features, device)
    
    # 5. è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆæ ¸å¿ƒå¾ªç¯ï¼ï¼‰
    print("=" * 80)
    print("ğŸ‹ï¸  å¼€å§‹è®­ç»ƒæ”»å‡»ç­–ç•¥ï¼ˆè§‚å¯Ÿæ ¸å¿ƒå¾ªç¯ï¼‰")
    print("=" * 80)
    print()
    
    num_episodes = 10  # ç®€åŒ–ç‰ˆï¼šåªè®­ç»ƒ10è½®
    
    for episode in range(num_episodes):
        print(f"ğŸ“ Episode {episode + 1}/{num_episodes}")
        print("-" * 60)
        
        # åˆ›å»ºç¯å¢ƒ
        env = AttackEnvironment(data, target_model, target_node, device)
        state = env.reset()
        
        episode_reward = 0
        done = False
        step = 0
        
        while not done and step < 5:
            # ğŸ”µ æ­¥éª¤1ï¼šè·å–å€™é€‰åŠ¨ä½œ
            candidates = env.get_candidates(budget=10)
            
            # ğŸ”µ æ­¥éª¤2ï¼šæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå¿ƒï¼‰
            action = agent.select_action(state, candidates)
            
            # ğŸ”µ æ­¥éª¤3ï¼šæ‰§è¡ŒåŠ¨ä½œï¼Œè§‚å¯Ÿç»“æœ
            next_state, reward, done, current_pred = env.step(action)
            
            # ğŸ”µ æ­¥éª¤4ï¼šæ›´æ–°Qç½‘ç»œï¼ˆå­¦ä¹ ï¼ï¼‰
            loss = agent.update(state, action, reward, next_state)
            
            episode_reward += reward
            
            print(f"   æ­¥éª¤{step + 1}: æ·»åŠ è¾¹{action} â†’ "
                  f"é¢„æµ‹={current_pred} | å¥–åŠ±={reward:.1f} | "
                  f"Q-loss={loss:.4f}")
            
            state = next_state
            step += 1
        
        print(f"   æ€»å¥–åŠ±: {episode_reward:.1f}")
        
        if episode_reward > 0:
            print("   âœ… æ”»å‡»æˆåŠŸï¼")
        else:
            print("   âŒ æ”»å‡»å¤±è´¥")
        print()
    
    # 6. æµ‹è¯•å­¦åˆ°çš„ç­–ç•¥
    print("=" * 80)
    print("ğŸš€ ç”¨å­¦åˆ°çš„ç­–ç•¥è¿›è¡Œæœ€ç»ˆæ”»å‡»")
    print("=" * 80)
    print()
    
    agent.epsilon = 0  # å…³é—­æ¢ç´¢ï¼Œçº¯åˆ©ç”¨
    env = AttackEnvironment(data, target_model, target_node, device)
    state = env.reset()
    
    done = False
    step = 0
    modifications = []
    
    while not done and step < 5:
        candidates = env.get_candidates(budget=10)
        action = agent.select_action(state, candidates)
        next_state, reward, done, current_pred = env.step(action)
        
        modifications.append(action)
        
        print(f"æ­¥éª¤{step + 1}:")
        print(f"  æ·»åŠ è¾¹: {action}")
        print(f"  å½“å‰é¢„æµ‹: {current_pred}")
        print(f"  å¥–åŠ±: {reward:.1f}")
        
        if done and reward > 0:
            print(f"\nğŸ‰ æ”»å‡»æˆåŠŸï¼")
            print(f"   ä¿®æ”¹è¾¹æ•°: {len(modifications)} æ¡")
            print(f"   æŸ¥è¯¢æ¬¡æ•°: {env.query_count} æ¬¡ âš¡")
            print(f"   é¢„æµ‹ä» {data.y[target_node].item()} â†’ {current_pred}")
            print(f"   å¹³å‡æ¯æ­¥æŸ¥è¯¢: {env.query_count / len(modifications):.1f} æ¬¡")
            break
        
        state = next_state
        step += 1
    
    if not done or reward < 0:
        print(f"\nâŒ æ”»å‡»å¤±è´¥ï¼ˆé¢„ç®—ä¸å¤Ÿæˆ–ç­–ç•¥æœªå­¦å¥½ï¼‰")
        print(f"   æ€»æŸ¥è¯¢æ¬¡æ•°: {env.query_count}")
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ æ ¸å¿ƒè§‚å¯Ÿ - RL-S2Vçš„ä¼˜åŠ¿")
    print("=" * 80)
    print("""
1. âš¡ æŸ¥è¯¢æ•ˆç‡ï¼ˆæœ€å¤§ä¼˜åŠ¿ï¼‰ï¼š
   - æ¯æ­¥åªæŸ¥è¯¢1æ¬¡é»‘ç›’æ¨¡å‹
   - å¯¹æ¯”ï¼šæ¢¯åº¦ä¼°ç®—æ–¹æ³•éœ€è¦æŸ¥è¯¢100+æ¬¡/æ­¥
   - å®æˆ˜æ„ä¹‰ï¼šæŸ¥è¯¢å°‘ = ä¸æ˜“è¢«æ£€æµ‹ = æ›´å¯è¡Œ
   
2. ğŸ”„ æ ¸å¿ƒå¾ªç¯ï¼š
   çŠ¶æ€è§‚å¯Ÿ â†’ é€‰æ‹©åŠ¨ä½œ â†’ è·å¾—å¥–åŠ± â†’ æ›´æ–°Qç½‘ç»œ â†’ é‡å¤
   
3. ğŸ® æ¢ç´¢vsåˆ©ç”¨ï¼š
   - è®­ç»ƒæ—¶ï¼š50%æ¢ç´¢ï¼ˆéšæœºï¼‰ï¼Œ50%åˆ©ç”¨ï¼ˆQå€¼æœ€å¤§ï¼‰
   - æµ‹è¯•æ—¶ï¼š100%åˆ©ç”¨ï¼ˆåªé€‰æœ€ä½³ï¼‰
   
4. ğŸ§  å­¦ä¹ èƒ½åŠ›ï¼š
   - Qç½‘ç»œé€æ¸å­¦ä¼š"å“ªç§è¾¹æœ€æœ‰æ•ˆ"
   - ä¸éœ€è¦æ¯æ¬¡éƒ½è¯„ä¼°æ‰€æœ‰å€™é€‰è¾¹
   
5. ğŸ•µï¸ é»‘ç›’ç‰¹æ€§ï¼š
   - åªæŸ¥è¯¢æ¨¡å‹è¾“å‡ºï¼ˆé¢„æµ‹ç»“æœï¼‰
   - ä¸éœ€è¦æ¢¯åº¦
   - æ›´æ¥è¿‘çœŸå®æ”»å‡»åœºæ™¯
   
6. ğŸ“Š æ•ˆç‡å¯¹æ¯”ï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   æ–¹æ³•      â”‚ æ¯æ­¥æŸ¥è¯¢   â”‚  é€‚ç”¨åœºæ™¯  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ RL-S2V      â”‚ 1æ¬¡ âš¡     â”‚ å®æˆ˜       â”‚
   â”‚ æ¢¯åº¦ä¼°ç®—    â”‚ 100+æ¬¡     â”‚ ç†è®ºç ”ç©¶   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    print("=" * 80)


if __name__ == '__main__':
    main()

