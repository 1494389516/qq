"""
é˜²å¾¡æ¼”ç¤ºï¼šé’ˆå¯¹epsilon=0.05-0.15çš„å¯¹æŠ—æ”»å‡»
å±•ç¤º3ç§å¸¸è§çš„é˜²å¾¡ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.datasets import Planetoid
import copy
import numpy as np

print("=" * 70)
print("ğŸ›¡ï¸  å›¾å¯¹æŠ—æ”»å‡»é˜²å¾¡æ¼”ç¤º")
print("=" * 70)

# ===== åŠ è½½æ•°æ®å’Œæ¨¡å‹ =====
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0]

class SimpleGCN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(dataset.num_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)
        
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# å¿«é€Ÿè®­ç»ƒä¸€ä¸ªåŸºç¡€æ¨¡å‹
print("\nğŸ‹ï¸  è®­ç»ƒåŸºç¡€æ¨¡å‹...")
model = SimpleGCN()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

model.train()
for epoch in range(100):
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

print("âœ“ åŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆ")

# æµ‹è¯•åŸå§‹å‡†ç¡®ç‡
model.eval()
with torch.no_grad():
    pred = model(data.x, data.edge_index).argmax(dim=1)
    correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()
    original_acc = correct / data.test_mask.sum().item()

print(f"âœ“ åŸå§‹å‡†ç¡®ç‡: {original_acc:.4f}")


# ===== FGSMæ”»å‡»å‡½æ•° =====
def fgsm_attack(model, data, epsilon=0.1):
    """æ ‡å‡†FGSMæ”»å‡»"""
    adv_data = copy.deepcopy(data)
    adv_data.x.requires_grad = True
    
    output = model(adv_data.x, adv_data.edge_index)
    target_node = data.test_mask.nonzero()[0].item()
    loss = F.nll_loss(output[target_node:target_node+1], 
                     adv_data.y[target_node:target_node+1])
    
    model.zero_grad()
    loss.backward()
    
    data_grad = adv_data.x.grad.data
    perturbed_x = adv_data.x + epsilon * data_grad.sign()
    adv_data.x = perturbed_x.detach()
    
    return adv_data


# ===== é˜²å¾¡ç­–ç•¥1ï¼šå¯¹æŠ—è®­ç»ƒ =====
print("\n" + "=" * 70)
print("ğŸ›¡ï¸  é˜²å¾¡ç­–ç•¥1ï¼šå¯¹æŠ—è®­ç»ƒ")
print("=" * 70)
print("åŸç†ï¼šåœ¨è®­ç»ƒæ—¶æ··å…¥å¯¹æŠ—æ ·æœ¬ï¼Œè®©æ¨¡å‹è§è¿‡æ”»å‡»")

class AdversarialTrainedGCN(nn.Module):
    """å¯¹æŠ—è®­ç»ƒçš„GCN"""
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(dataset.num_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)
        
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# å¯¹æŠ—è®­ç»ƒ
print("\nè®­ç»ƒä¸­ï¼ˆæ··å…¥å¯¹æŠ—æ ·æœ¬ï¼‰...")
adv_model = AdversarialTrainedGCN()
optimizer = torch.optim.Adam(adv_model.parameters(), lr=0.01, weight_decay=5e-4)

for epoch in range(100):
    adv_model.train()
    
    # 1. æ­£å¸¸è®­ç»ƒ
    optimizer.zero_grad()
    out = adv_model(data.x, data.edge_index)
    loss_clean = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    
    # 2. ç”Ÿæˆå¯¹æŠ—æ ·æœ¬å¹¶è®­ç»ƒï¼ˆé‡ç‚¹ï¼ï¼‰
    adv_data = fgsm_attack(adv_model, data, epsilon=0.1)  # ç”¨0.1è®­ç»ƒ
    out_adv = adv_model(adv_data.x, adv_data.edge_index)
    loss_adv = F.nll_loss(out_adv[data.train_mask], data.y[data.train_mask])
    
    # 3. ç»¼åˆæŸå¤±
    total_loss = 0.5 * loss_clean + 0.5 * loss_adv
    total_loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 50 == 0:
        print(f"  Epoch {epoch+1}/100, æŸå¤±={total_loss.item():.4f}")

print("âœ“ å¯¹æŠ—è®­ç»ƒå®Œæˆ")

# æµ‹è¯•å¯¹æŠ—è®­ç»ƒæ¨¡å‹
adv_model.eval()
with torch.no_grad():
    pred = adv_model(data.x, data.edge_index).argmax(dim=1)
    correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()
    adv_train_acc = correct / data.test_mask.sum().item()

print(f"âœ“ å¯¹æŠ—è®­ç»ƒæ¨¡å‹åŸå§‹å‡†ç¡®ç‡: {adv_train_acc:.4f}")


# ===== é˜²å¾¡ç­–ç•¥2ï¼šç‰¹å¾æ‰°åŠ¨æ£€æµ‹ =====
print("\n" + "=" * 70)
print("ğŸ›¡ï¸  é˜²å¾¡ç­–ç•¥2ï¼šç‰¹å¾æ‰°åŠ¨æ£€æµ‹")
print("=" * 70)
print("åŸç†ï¼šæ£€æµ‹ç‰¹å¾çš„å¼‚å¸¸å˜åŒ–ï¼ˆç»Ÿè®¡æ–¹æ³•ï¼‰")

def detect_perturbation(x, threshold=0.1):
    """
    æ£€æµ‹ç‰¹å¾æ˜¯å¦è¢«æ‰°åŠ¨
    æ–¹æ³•ï¼šè®¡ç®—ç‰¹å¾å˜åŒ–çš„ç»Ÿè®¡ç‰¹æ€§
    """
    # è®¡ç®—ç‰¹å¾çš„æ ‡å‡†å·®
    feature_std = x.std(dim=0)
    
    # æ£€æµ‹å¼‚å¸¸å¤§çš„æ ‡å‡†å·®ï¼ˆå¯èƒ½æ˜¯å¯¹æŠ—æ‰°åŠ¨ï¼‰
    suspicious = (feature_std > threshold).sum().item()
    
    return suspicious > len(feature_std) * 0.1  # å¦‚æœ10%ä»¥ä¸Šç‰¹å¾å¼‚å¸¸

# ===== é˜²å¾¡ç­–ç•¥3ï¼šæ¢¯åº¦é®è”½ =====
print("\n" + "=" * 70)
print("ğŸ›¡ï¸  é˜²å¾¡ç­–ç•¥3ï¼šæ¢¯åº¦é®è”½ï¼ˆè¾“å…¥éšæœºåŒ–ï¼‰")
print("=" * 70)
print("åŸç†ï¼šåœ¨è¾“å…¥ä¸­åŠ å…¥éšæœºå™ªå£°ï¼Œç ´åæ¢¯åº¦è®¡ç®—")

def gradient_masking_defense(model, x, edge_index, noise_scale=0.01):
    """
    æ¢¯åº¦é®è”½é˜²å¾¡
    åœ¨è¾“å…¥ä¸­åŠ å…¥éšæœºå™ªå£°
    """
    # åŠ å…¥éšæœºå™ªå£°
    noise = torch.randn_like(x) * noise_scale
    x_noisy = x + noise
    
    # é¢„æµ‹ï¼ˆå¤šæ¬¡é¢„æµ‹å–å¹³å‡ï¼‰
    predictions = []
    for _ in range(5):  # é¢„æµ‹5æ¬¡
        noise = torch.randn_like(x) * noise_scale
        x_temp = x + noise
        pred = model(x_temp, edge_index)
        predictions.append(pred)
    
    # å¹³å‡é¢„æµ‹
    avg_pred = torch.stack(predictions).mean(dim=0)
    return avg_pred


# ===== æµ‹è¯•3ç§é˜²å¾¡æ•ˆæœ =====
print("\n" + "=" * 70)
print("ğŸ“Š æµ‹è¯•ï¼šé’ˆå¯¹epsilon=0.05-0.15çš„é˜²å¾¡æ•ˆæœ")
print("=" * 70)

# å±é™©åŒºé—´
dangerous_epsilons = [0.05, 0.08, 0.1, 0.12, 0.15]

print(f"\n{'Epsilon':<10} {'åŸºç¡€æ¨¡å‹':<12} {'å¯¹æŠ—è®­ç»ƒ':<12} {'æ¢¯åº¦é®è”½':<12}")
print("-" * 70)

for eps in dangerous_epsilons:
    # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
    adv_data = fgsm_attack(model, data, epsilon=eps)
    
    # 1. åŸºç¡€æ¨¡å‹ï¼ˆæ— é˜²å¾¡ï¼‰
    with torch.no_grad():
        pred = model(adv_data.x, adv_data.edge_index).argmax(dim=1)
        correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()
        acc_base = correct / data.test_mask.sum().item()
    
    # 2. å¯¹æŠ—è®­ç»ƒæ¨¡å‹
    with torch.no_grad():
        pred = adv_model(adv_data.x, adv_data.edge_index).argmax(dim=1)
        correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()
        acc_adv_train = correct / data.test_mask.sum().item()
    
    # 3. æ¢¯åº¦é®è”½
    with torch.no_grad():
        out = gradient_masking_defense(model, adv_data.x, adv_data.edge_index)
        pred = out.argmax(dim=1)
        correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()
        acc_grad_mask = correct / data.test_mask.sum().item()
    
    print(f"{eps:<10.2f} {acc_base:<12.4f} {acc_adv_train:<12.4f} {acc_grad_mask:<12.4f}")


# ===== æ€»ç»“ =====
print("\n" + "=" * 70)
print("ğŸ“Š é˜²å¾¡æ•ˆæœæ€»ç»“")
print("=" * 70)

print("""
âœ¨ å…³é”®å‘ç°ï¼š

1. ã€å¯¹æŠ—è®­ç»ƒã€‘æœ€æœ‰æ•ˆ
   - åœ¨è®­ç»ƒæ—¶æ··å…¥æ”»å‡»æ ·æœ¬
   - è®©æ¨¡å‹"è§è¿‡ä¸–é¢"
   - å‡†ç¡®ç‡ä¸‹é™æœ€å°
   - æ¨èæŒ‡æ•°ï¼šâ­â­â­â­â­

2. ã€æ¢¯åº¦é®è”½ã€‘æ¬¡ä¹‹
   - ç ´åæ¢¯åº¦ä¿¡æ¯
   - è®©æ”»å‡»è€…æ— æ³•è®¡ç®—æ¢¯åº¦
   - ä½†è®¡ç®—æˆæœ¬é«˜ï¼ˆéœ€è¦å¤šæ¬¡é¢„æµ‹ï¼‰
   - æ¨èæŒ‡æ•°ï¼šâ­â­â­

3. ã€æ‰°åŠ¨æ£€æµ‹ã€‘è¾…åŠ©æ‰‹æ®µ
   - æ£€æµ‹å¼‚å¸¸è¾“å…¥
   - é…åˆå…¶ä»–æ–¹æ³•ä½¿ç”¨
   - æ¨èæŒ‡æ•°ï¼šâ­â­â­

ğŸ’¡ å®æˆ˜å»ºè®®ï¼š
- æ ¸å¿ƒï¼šå¯¹æŠ—è®­ç»ƒï¼ˆå¿…é€‰ï¼‰
- è¾…åŠ©ï¼šå¼‚å¸¸æ£€æµ‹ + ä¸šåŠ¡è§„åˆ™
- ç›‘æ§ï¼šæŒç»­ç›‘æ§æ”»å‡»æ¨¡å¼å˜åŒ–
""")

print("\nğŸ¯ é’ˆå¯¹epsilon=0.05-0.15çš„é˜²å¾¡é‡ç‚¹ï¼š")
print("  1. è¿™ä¸ªèŒƒå›´æœ€å±é™©ï¼ˆæ”»å‡»æ•ˆæœå¥½ + éšè”½ï¼‰")
print("  2. å¯¹æŠ—è®­ç»ƒæ—¶é‡ç‚¹ç”¨è¿™ä¸ªèŒƒå›´çš„epsilon")
print("  3. å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿè®¾ç½®é˜ˆå€¼è¦è€ƒè™‘è¿™ä¸ªèŒƒå›´")
print("  4. æŒç»­ç›‘æ§ï¼šé»‘äº§ä¼šè¯•æ¢æœ€ä¼˜epsilon")

print("\n" + "=" * 70)
print("âœ… é˜²å¾¡æ¼”ç¤ºå®Œæˆï¼")
print("=" * 70)
