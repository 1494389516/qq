# 📚 图对抗攻击经典论文汇总与解读

整理时间：2025年10月

---

## 🎯 论文分类

### 一、基础攻击方法（必读）
### 二、黑盒攻击方法（实战）
### 三、综述论文（入门）
### 四、防御方法
### 五、应用论文（风控相关）

---

## 📖 一、基础攻击方法（必读）

### 1. Nettack (KDD 2018) ⭐⭐⭐⭐⭐

**论文标题：**
《Adversarial Attacks on Neural Networks for Graph Data》

**作者：** Daniel Zügner, Amir Akbarnejad, Stephan Günnemann (慕尼黑工业大学)

**arxiv链接：** https://arxiv.org/abs/1805.07984

**GitHub代码：** https://github.com/danielzuegner/nettack

**核心贡献：**
- 首个针对图神经网络的对抗攻击研究
- 提出针对节点分类的目标攻击（targeted attack）
- 同时攻击图结构和节点特征
- 保持图的离散性约束

**攻击方法：**
```
1. 定义攻击目标：改变目标节点的分类结果
2. 搜索空间：边的添加/删除 + 特征修改
3. 贪心算法：每次选择影响最大的一个修改
4. 约束条件：保持度分布、特征协方差等
```

**关键公式：**
```
攻击优化目标：
max L(θ*, A', X')
s.t. ||A - A'||₀ ≤ Δ_A  (结构扰动预算)
     ||X - X'||₀ ≤ Δ_X  (特征扰动预算)
```

**实验结果：**
- 在Cora数据集上，仅修改5条边
- 目标节点分类准确率从90%降到10%
- 攻击成功率高达90%以上

**适用场景：**
- 白盒攻击（需要知道模型参数）
- 单节点目标攻击
- 学术研究和系统测试

**为什么重要：**
这是第一篇，奠定了图对抗攻击的基础范式！

---

### 2. Metattack (ICLR 2019) ⭐⭐⭐⭐⭐

**论文标题：**
《Adversarial Attacks on Graph Neural Networks via Meta Learning》

**作者：** Daniel Zügner, Stephan Günnemann

**arxiv链接：** https://arxiv.org/abs/1902.08412

**GitHub代码：** https://github.com/danielzuegner/gnn-meta-attack

**核心贡献：**
- 全局攻击（poisoning attack）：训练时投毒
- 使用元学习（meta-learning）近似攻击效果
- 影响所有节点的分类性能
- 更接近真实攻击场景

**与Nettack的区别：**
```
Nettack：
- 目标攻击（攻击单个节点）
- 测试时攻击（evasion attack）

Metattack：
- 全局攻击（攻击整个模型）
- 训练时投毒（poisoning attack）
- 更难防御
```

**攻击流程：**
```
1. 初始化干净的图 G = (A, X)
2. 选择要修改的边/特征
3. 用元学习估计修改后的效果：
   θ* ≈ θ - α∇L_train  (快速近似重训练)
4. 选择让测试损失最大的修改
5. 重复直到达到扰动预算
```

**实验结果：**
- 修改图中1%的边
- 整体分类准确率下降15-25%
- 比Nettack更隐蔽（全局影响分散）

**适用场景：**
- 数据投毒攻击
- 影响模型训练阶段
- 更接近真实黑产行为

---

### 3. Topology Attack (NeurIPS 2019) ⭐⭐⭐⭐

**论文标题：**
《Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective》

**作者：** Kaidi Xu, Hongge Chen, et al. (MIT-IBM Watson AI Lab)

**arxiv链接：** https://arxiv.org/abs/1906.04214

**核心贡献：**
- 统一的优化框架
- PGD攻击在图上的应用
- 同时考虑攻击和防御

**攻击方法：**
```python
# 投影梯度下降（PGD）在图上
for step in range(T):
    # 1. 梯度下降
    A' = A' + α * ∇_A L(A', X)
    
    # 2. 投影回可行域（保持图的性质）
    A' = Project(A', Δ)  # 投影到扰动预算内
    A' = Symmetrize(A')   # 保持对称性
    A' = Binarize(A')     # 保持0/1性质
```

**特点：**
- 理论上更严格
- 提供了攻防的统一视角
- 计算效率高

---

## 🕵️ 二、黑盒攻击方法（实战）

### 4. RL-S2V (IJCAI 2019) ⭐⭐⭐⭐

**论文标题：**
《Adversarial Attack on Graph Structured Data》

**作者：** Hanjun Dai, Hui Li, et al. (Georgia Tech)

**arxiv链接：** https://arxiv.org/abs/1806.02371

**GitHub代码：** https://github.com/Hanjun-Dai/graph_adversarial_attack

**核心贡献：**
- 第一个用强化学习做图对抗攻击
- 黑盒设置（不需要知道目标模型）
- 可迁移到不同的GNN模型

**方法：**
```
强化学习框架：
- 状态：当前图的状态
- 动作：添加或删除一条边
- 奖励：目标模型的分类错误

使用Q-learning训练攻击策略：
Q(s, a) = 攻击成功的期望奖励
```

**训练过程：**
```python
# 伪代码
for episode in range(N):
    state = 初始图
    for step in range(max_step):
        # 选择动作（加/删边）
        action = policy.select(state)
        
        # 执行动作
        new_state = apply(state, action)
        
        # 查询目标模型
        reward = check_attack_success(new_state)
        
        # 更新Q值
        Q.update(state, action, reward)
```

**优势：**
- ✅ 不需要梯度
- ✅ 黑盒攻击
- ✅ 学习通用攻击策略
- ✅ 可迁移

**劣势：**
- ❌ 训练成本高
- ❌ 需要多次查询目标模型

**适用场景：**
- 黑盒攻击
- 有查询次数限制但可以多次尝试
- 想找到通用攻击模式

---

### 5. ReWatt (AAAI 2021) ⭐⭐⭐⭐

**论文标题：**
《Adversarial Attack on Large Scale Graph》

**作者：** Jintang Li, et al.

**arxiv链接：** https://arxiv.org/abs/2009.03488

**核心贡献：**
- 查询高效的黑盒攻击
- 可扩展到大规模图
- 只需要少量查询

**方法：**
```
核心思想：使用梯度估计

1. 用有限差分估计梯度：
   ∇f(x) ≈ [f(x+δ) - f(x-δ)] / 2δ

2. 随机采样方向：
   只在重要方向上估计梯度

3. 贪心选择：
   每次选择影响最大的边修改
```

**查询效率：**
```
传统黑盒攻击：需要10,000+次查询
ReWatt：只需要100-500次查询
→ 提升20-100倍效率
```

**适用场景：**
- 黑盒攻击
- 查询次数受限（风控系统会限制查询）
- 大规模图

---

### 6. Transferable Attack (2022) ⭐⭐⭐⭐

**论文标题：**
《More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks》

**arxiv链接：** https://arxiv.org/abs/2202.03195

**核心思想：**
```
在替代模型上生成攻击
→ 迁移到目标模型

流程：
1. 收集公开数据，训练替代模型
2. 在替代模型上用白盒攻击生成对抗样本
3. 直接用于攻击真实的黑盒模型
```

**迁移成功率：**
```
同类型GNN（GCN → GCN）：70-80%
不同类型（GCN → GAT）：50-60%
不同架构（GCN → GraphSAGE）：40-50%
```

**实战技巧：**
- 用集成模型作为替代模型（多个GNN的集成）
- 对抗训练增强迁移性
- 在多个替代模型上都成功的攻击迁移性更强

---

## 📚 三、综述论文（入门必读）

### 7. 中山大学综述 (2020) ⭐⭐⭐⭐⭐

**论文标题：**
《图对抗机器学习：算法与应用》

**中文！** 非常适合入门

**内容涵盖：**
- 40+种攻击方法
- 30+种防御方法
- 分类清晰，讲解详细

**获取方式：**
- 搜索"图对抗机器学习 中山大学"
- 或访问：https://www.zhuanzhi.ai/document/a0e0509afb4cc86d409d9c829eb4d615

---

### 8. Survey Paper (TNNLS 2021) ⭐⭐⭐⭐⭐

**论文标题：**
《Adversarial Attacks and Defenses on Graphs: A Survey》

**作者：** Liang Chen, et al.

**arxiv链接：** https://arxiv.org/abs/2003.00653

**内容：**
- 系统化的分类框架
- 攻击：目标攻击 vs 全局攻击
- 防御：对抗训练、图净化、鲁棒架构
- 200+篇参考文献

**推荐阅读顺序：**
1. 先看Introduction和Taxonomy（分类）
2. 再看感兴趣的具体方法
3. 最后看Future Directions（未来方向）

---

## 🛡️ 四、防御方法

### 9. GCN-Jaccard (WWW 2019) ⭐⭐⭐

**论文标题：**
《Adversarial Examples on Graph Data: Deep Insights into Attack and Defense》

**arxiv链接：** https://arxiv.org/abs/1903.01610

**核心思想：**
```
检测并删除可疑的边

方法：
1. 计算节点间的Jaccard相似度：
   J(u,v) = |N(u) ∩ N(v)| / |N(u) ∪ N(v)|

2. 删除相似度低的边（可疑连接）

3. 在净化后的图上训练GNN
```

**效果：**
- 简单有效
- 对Nettack攻击有很好的防御
- 但可能误删正常的边

---

### 10. Pro-GNN (KDD 2020) ⭐⭐⭐⭐

**论文标题：**
《Graph Structure Learning for Robust Graph Neural Networks》

**arxiv链接：** https://arxiv.org/abs/2005.10203

**GitHub代码：** https://github.com/ChandlerBang/Pro-GNN

**核心思想：**
```
联合优化图结构和GNN参数

优化目标：
min L_task + λ₁R_sparse + λ₂R_low_rank + λ₃R_feature
 S,Θ

其中：
- S：优化后的图结构（邻接矩阵）
- Θ：GNN参数
- R_sparse：稀疏性约束
- R_low_rank：低秩约束（图有社区结构）
- R_feature：特征相似性约束
```

**优势：**
- 端到端学习鲁棒的图结构
- 同时修正攻击和原始噪声
- SOTA防御效果

---

## 💼 五、应用论文（风控相关）

### 11. 金融欺诈检测 (CIKM 2021) ⭐⭐⭐⭐

**论文标题：**
《Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection》

**arxiv链接：** https://arxiv.org/abs/2005.00625

**场景：**
- 蚂蚁金服的真实欺诈检测场景
- 用户-设备-商家 多类型节点图
- 欺诈团伙会伪造关系

**问题：**
```
GNN假设：相连节点相似（同质性）
  ↓
欺诈场景：欺诈者会连接正常用户（伪装）
  ↓
GNN被误导！
```

**解决方案：**
- 标签感知的注意力机制
- 区分同质边和异质边
- 降低可疑边的权重

**启示：**
风控中必须考虑对手的适应性攻击！

---

### 12. 社交网络虚假账号 (WWW 2020) ⭐⭐⭐

**论文标题：**
《GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection》

**应用：**
- 社交网络虚假账号检测
- 水军团伙识别

**攻击模式：**
```
水军策略：
1. 批量注册账号
2. 互相关注（构建虚假关系网）
3. 关注正常用户（伪装）
4. 刷量、刷评论
```

**检测方法：**
- 用GNN学习用户表示
- 异常检测模块
- 抗对抗训练

---

## 🔗 资源汇总

### arXiv搜索关键词：
```
- "adversarial attack graph neural network"
- "robust graph neural network"
- "graph adversarial learning"
- "poisoning attack graph"
- "evasion attack GNN"
```

### 代码库：

1. **DeepRobust** (推荐！)
   - GitHub: https://github.com/DSE-MSU/DeepRobust
   - 包含：10+种攻击，8+种防御
   - 文档完善，易上手

2. **Nettack官方实现**
   - GitHub: https://github.com/danielzuegner/nettack
   - 经典必看

3. **Pro-GNN官方实现**
   - GitHub: https://github.com/ChandlerBang/Pro-GNN
   - 最强防御之一

### 数据集：

常用的图数据集：
```
节点分类：
- Cora (2708节点，5429边，7类)
- Citeseer (3327节点，4732边，6类)
- Pubmed (19717节点，44338边，3类)

大规模：
- OGB数据集 (Open Graph Benchmark)
- Reddit (232,965节点)

风控相关：
- Yelp欺诈检测数据集
- Amazon评论数据集
```

---

## 📖 推荐阅读路径

### 入门路径（1-2周）：
```
Day 1-2: 看综述论文，建立框架
Day 3-5: 读Nettack，理解基础攻击
Day 6-7: 跑DeepRobust代码，实践
```

### 进阶路径（2-4周）：
```
Week 1: Nettack + Metattack（白盒攻击）
Week 2: RL-S2V + ReWatt（黑盒攻击）
Week 3: GCN-Jaccard + Pro-GNN（防御）
Week 4: 实战项目（风控场景）
```

### 深入研究（持续）：
```
- 关注顶会（KDD, ICLR, NeurIPS, WWW）
- 订阅arXiv更新（每周新论文）
- 参加比赛（KDD Cup, CIKM AnalytiCup）
- 阅读最新论文（2023-2024）
```

---

## 📝 论文下载tips

### 如何找论文：

1. **arXiv**（最快）
   - 访问：https://arxiv.org
   - 搜索论文标题
   - 直接下载PDF

2. **Google Scholar**
   - 搜索论文标题
   - 点击"PDF"链接
   - 或找到arXiv链接

3. **会议官网**
   - KDD: https://kdd.org
   - WWW: https://www.thewebconf.org
   - NeurIPS: https://neurips.cc

4. **GitHub**
   - 很多论文作者会在GitHub放论文PDF
   - 代码仓库通常包含论文链接

### 如何阅读：

```
第一遍（30分钟）：
- 看标题、摘要、结论
- 看图表
- 判断是否值得细读

第二遍（1-2小时）：
- 详细读方法部分
- 理解算法流程
- 看实验结果

第三遍（2-4小时）：
- 对照代码理解细节
- 推导公式
- 复现实验
```

---

## 🎯 重点论文速查表

| 论文 | 年份 | 类型 | 重要性 | 难度 | 推荐阅读顺序 |
|------|------|------|--------|------|------------|
| Nettack | 2018 | 白盒攻击 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 1 |
| Metattack | 2019 | 全局攻击 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 2 |
| RL-S2V | 2019 | 黑盒攻击 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 4 |
| GCN-Jaccard | 2019 | 防御 | ⭐⭐⭐ | ⭐⭐ | 3 |
| Pro-GNN | 2020 | 防御 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 5 |
| Survey | 2021 | 综述 | ⭐⭐⭐⭐⭐ | ⭐⭐ | 0（先读）|
| ReWatt | 2021 | 黑盒攻击 | ⭐⭐⭐⭐ | ⭐⭐⭐ | 6 |

---

**整理完毕！保存此文档，随时查阅！** 📚

有任何论文想要详细解读，随时告诉我！
