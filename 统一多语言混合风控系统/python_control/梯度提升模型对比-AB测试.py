#!/usr/bin/env python3

"""
æ¢¯åº¦æå‡æ¨¡å‹å¯¹æ¯”ç³»ç»Ÿ - XGBoost vs LightGBM vs éšæœºæ£®æ—
åŸºäºé£æ§ç®—æ³•ä¸“å®¶çš„ç†è®ºæ¡†æ¶å’Œæ•°å­¦å»ºæ¨¡è§†è§’

æ ¸å¿ƒç®—æ³•ç†è®ºï¼š
1. æ¢¯åº¦æå‡å†³ç­–æ ‘ (GBDT) ç†è®ºå¯¹æ¯”
2. è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
3. A/Bæµ‹è¯•ç»Ÿè®¡å­¦æ¡†æ¶
4. å¤šç›®æ ‡ä¼˜åŒ–ä¸é›†æˆå­¦ä¹ 
5. ä¸šåŠ¡åœºæ™¯é©±åŠ¨çš„æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç­–ç•¥
6. VPNæ£€æµ‹ä¸ç½‘ç»œå¨èƒåˆ†æé›†æˆ
7. å¤šå±‚æ¬¡é£æ§æ£€æµ‹èåˆç³»ç»Ÿ
"""

import time
import numpy as np
import pandas as pd
import warnings
import logging
import threading
import queue
import hashlib
from typing import Dict, List, Tuple, Optional, Any, cast
from dataclasses import dataclass
from enum import Enum
from collections import defaultdict, deque
from datetime import datetime, timedelta
from abc import ABC, abstractmethod
import copy

# æ ¸å¿ƒæœºå™¨å­¦ä¹ åº“
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("è­¦å‘Š: XGBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡XGBoostæ¨¡å‹")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("è­¦å‘Š: LightGBMæœªå®‰è£…ï¼Œå°†è·³è¿‡LightGBMæ¨¡å‹")

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
from scipy import stats

warnings.filterwarnings('ignore')


class AttackType(Enum):
    """æ”»å‡»ç±»å‹æšä¸¾"""
    NORMAL = 0
    CRAWLER = 1
    BRUTE_FORCE = 2
    ORDER_FRAUD = 3
    PAYMENT_FRAUD = 4
    DDOS = 5
    VPN_TUNNEL = 6
    NETWORK_ANOMALY = 7


class VPNType(Enum):
    """VPNç±»å‹æšä¸¾"""
    OPENVPN = "OpenVPN"
    IPSEC = "IPSec"
    WIREGUARD = "WireGuard"
    PPTP = "PPTP"
    L2TP = "L2TP"
    SSTP = "SSTP"
    UNKNOWN = "Unknown"


@dataclass
class NetworkPacket:
    """ç½‘ç»œæ•°æ®åŒ…ç»“æ„"""
    timestamp: float
    src_ip: str
    dst_ip: str
    src_port: int
    dst_port: int
    protocol: str
    size: int
    direction: str  # 'up' or 'down'
    payload_size: int
    tls_info: Optional[Dict] = None


@dataclass
class NetworkFlow:
    """ç½‘ç»œæµç»“æ„"""
    flow_id: str
    packets: List[NetworkPacket]
    start_time: float
    end_time: float
    src_ip: str
    dst_ip: str


@dataclass
class NetworkDetectionResult:
    """ç½‘ç»œæ£€æµ‹ç»“æœ"""
    flow_id: str
    is_threat: bool
    threat_type: str
    confidence: float
    detection_stage: str
    features: Dict[str, Any]
    timestamp: float


@dataclass 
class ModelPerformanceMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    auc_score: float
    training_time: float
    

@dataclass
class ABTestResult:
    """A/Bæµ‹è¯•ç»“æœ"""
    model_a_name: str
    model_b_name: str
    sample_size: int
    p_value: float
    effect_size: float
    is_significant: bool
    winner: str
    confidence_interval: Tuple[float, float]


class AdvancedDataGenerator:
    """é«˜çº§æ•°æ®ç”Ÿæˆå™¨ - åŸºäºé£æ§åœºæ™¯çš„ç‰¹å¾å·¥ç¨‹"""
    
    @staticmethod
    def generate_features_matrix(attack_type: AttackType, count: int) -> np.ndarray:
        """ç”Ÿæˆç‰¹å¾çŸ©é˜µ - åŸºäºæ”»å‡»ç‰¹å¾å·¥ç¨‹è§„èŒƒ"""
        np.random.seed(42 + attack_type.value)
        
        if attack_type == AttackType.NORMAL:
            # æ­£å¸¸æµé‡ç‰¹å¾
            features = np.random.multivariate_normal(
                mean=[15, 12, 500, 2, 0.8, 0.13, 33, 12, 3, 0.5, 0.3, 50, 0.8, 0.9],
                cov=np.diag([9, 4, 2500, 1, 0.04, 0.01, 100, 36, 9, 0.25, 0.09, 2500, 0.04, 0.01]),
                size=count
            )
        elif attack_type == AttackType.CRAWLER:
            # çˆ¬è™«æ”»å‡»ç‰¹å¾ï¼šé«˜PVï¼Œä½è½¬åŒ–
            features = np.random.multivariate_normal(
                mean=[8, 2, 2000, 8, 0.25, 1.0, 250, 12, 3, 0.5, 0.3, 100, 0.3, 0.4],
                cov=np.diag([4, 1, 90000, 4, 0.01, 0.04, 2500, 36, 9, 0.25, 0.09, 10000, 0.04, 0.04]),
                size=count
            )
        elif attack_type == AttackType.BRUTE_FORCE:
            # æš´åŠ›ç ´è§£ç‰¹å¾ï¼šé«˜è¯·æ±‚ï¼Œæä½æˆåŠŸç‡
            features = np.random.multivariate_normal(
                mean=[80, 2, 300, 25, 0.025, 0.31, 3.75, 12, 3, 0.5, 0.7, 200, 0.2, 0.2],
                cov=np.diag([225, 1, 2500, 25, 0.0001, 0.01, 1, 36, 9, 0.25, 0.09, 10000, 0.04, 0.04]),
                size=count
            )
        elif attack_type == AttackType.ORDER_FRAUD:
            # è®¢å•æ¬ºè¯ˆç‰¹å¾ï¼šå¼‚å¸¸è®¢å•æ¨¡å¼
            features = np.random.multivariate_normal(
                mean=[40, 35, 600, 12, 0.875, 0.3, 15, 12, 3, 0.5, 0.3, 150, 0.55, 0.65],
                cov=np.diag([64, 25, 10000, 9, 0.01, 0.01, 25, 36, 9, 0.25, 0.09, 6400, 0.04, 0.04]),
                size=count
            )
        elif attack_type == AttackType.PAYMENT_FRAUD:
            # æ”¯ä»˜æ¬ºè¯ˆç‰¹å¾ï¼šæ”¯ä»˜ç¯èŠ‚å¼‚å¸¸
            features = np.random.multivariate_normal(
                mean=[25, 3, 400, 18, 0.12, 0.72, 16, 12, 3, 0.5, 0.3, 300, 0.45, 0.55],
                cov=np.diag([25, 4, 3600, 16, 0.01, 0.04, 16, 36, 9, 0.25, 0.09, 22500, 0.04, 0.04]),
                size=count
            )
        else:  # DDOS
            # DDoSæ”»å‡»ç‰¹å¾ï¼šæé«˜æµé‡
            features = np.random.multivariate_normal(
                mean=[200, 1, 8000, 50, 0.005, 0.25, 40, 12, 3, 0.5, 0.3, 500, 0.15, 0.25],
                cov=np.diag([1600, 1, 1000000, 100, 0.00001, 0.01, 100, 36, 9, 0.25, 0.09, 40000, 0.04, 0.04]),
                size=count
            )
        
        # ç¡®ä¿éè´Ÿå€¼
        features = np.abs(features)
        
        # æ·»åŠ æ—¶é—´è¶‹åŠ¿ç‰¹å¾
        trends = np.random.normal(0, [0.5, 0.3, 2, 0.2], (count, 4))
        features = np.hstack([features, trends])
        
        return features
    
    @staticmethod
    def generate_balanced_dataset(total_samples: int = 2000) -> Tuple[np.ndarray, np.ndarray]:
        """ç”Ÿæˆå¹³è¡¡æ•°æ®é›†"""
        # å„æ”»å‡»ç±»å‹æ ·æœ¬åˆ†é…
        samples_per_class = {
            AttackType.NORMAL: int(total_samples * 0.6),
            AttackType.CRAWLER: int(total_samples * 0.15),
            AttackType.BRUTE_FORCE: int(total_samples * 0.1),
            AttackType.ORDER_FRAUD: int(total_samples * 0.08),
            AttackType.PAYMENT_FRAUD: int(total_samples * 0.04),
            AttackType.DDOS: int(total_samples * 0.03)
        }
        
        X_list: List[np.ndarray] = []
        y_list: List[np.ndarray] = []
        
        for attack_type, count in samples_per_class.items():
            if count > 0:
                features = AdvancedDataGenerator.generate_features_matrix(attack_type, count)
                labels = np.full(count, attack_type.value)
                
                X_list.append(features)
                y_list.append(labels)
        
        X = np.vstack(X_list)
        y = np.hstack(y_list)
        
        # éšæœºæ‰“ä¹±
        indices = np.random.permutation(len(X))
        X = X[indices]
        y = y[indices]
        
        return X, y


class GradientBoostingComparator:
    """æ¢¯åº¦æå‡ç®—æ³•æ¯”è¾ƒå™¨"""
    
    def __init__(self):
        self.models = {}
        self.performance_metrics = {}
        self.feature_names = [
            'order_requests', 'payment_success', 'product_pv', 'risk_hits',
            'payment_success_rate', 'risk_hit_rate', 'pv_order_ratio',
            'hour_of_day', 'day_of_week', 'is_weekend', 'is_night',
            'time_offset', 'source_entropy', 'ip_entropy',
            'order_trend', 'payment_trend', 'pv_trend', 'risk_trend'
        ]
        
        # æ—¥å¿—é…ç½®
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def train_xgboost(self, X_train: np.ndarray, y_train: np.ndarray, 
                     X_val: np.ndarray, y_val: np.ndarray) -> Optional[ModelPerformanceMetrics]:
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        if not XGBOOST_AVAILABLE:
            self.logger.warning("XGBoostä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒ")
            return None
            
        self.logger.info("å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        start_time = time.time()
        
        # XGBoostæ¨¡å‹é…ç½®
        import xgboost as xgb_module
        model = xgb_module.XGBClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=1.0,
            objective='multi:softprob',
            eval_metric='mlogloss',
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        
        training_time = time.time() - start_time
        
        # è¯„ä¼°æ€§èƒ½ - æ·»åŠ ç±»å‹è½¬æ¢
        y_pred = model.predict(X_val)
        y_proba = model.predict_proba(X_val)
        
        # æ˜ç¡®çš„ç±»å‹è½¬æ¢
        metrics = self._calculate_metrics(y_val, cast(np.ndarray, y_pred), cast(np.ndarray, y_proba), training_time)
        
        self.models['XGBoost'] = model
        self.performance_metrics['XGBoost'] = metrics
        
        self.logger.info(f"XGBoostè®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {metrics.accuracy:.4f}, F1: {metrics.f1_score:.4f}")
        return metrics
        
    def train_lightgbm(self, X_train: np.ndarray, y_train: np.ndarray,
                      X_val: np.ndarray, y_val: np.ndarray) -> Optional[ModelPerformanceMetrics]:
        """è®­ç»ƒLightGBMæ¨¡å‹"""
        if not LIGHTGBM_AVAILABLE:
            self.logger.warning("LightGBMä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒ")
            return None
            
        self.logger.info("å¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹...")
        start_time = time.time()
        
        # LightGBMæ¨¡å‹é…ç½®
        import lightgbm as lgb_module
        model = lgb_module.LGBMClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            num_leaves=31,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=1.0,
            objective='multiclass',
            metric='multi_logloss',
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            callbacks=[lgb_module.early_stopping(20), lgb_module.log_evaluation(0)]
        )
        
        training_time = time.time() - start_time
        
        # è¯„ä¼°æ€§èƒ½
        y_pred = model.predict(X_val)
        y_proba = model.predict_proba(X_val)
        
        metrics = self._calculate_metrics(y_val, cast(np.ndarray, y_pred), cast(np.ndarray, y_proba), training_time)
        
        self.models['LightGBM'] = model
        self.performance_metrics['LightGBM'] = metrics
        
        self.logger.info(f"LightGBMè®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {metrics.accuracy:.4f}, F1: {metrics.f1_score:.4f}")
        return metrics
        
    def train_random_forest(self, X_train: np.ndarray, y_train: np.ndarray,
                           X_val: np.ndarray, y_val: np.ndarray) -> ModelPerformanceMetrics:
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        start_time = time.time()
        
        # éšæœºæ£®æ—æ¨¡å‹é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            bootstrap=True,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        
        # è¯„ä¼°æ€§èƒ½
        y_pred = model.predict(X_val)
        y_proba = model.predict_proba(X_val)
        
        metrics = self._calculate_metrics(y_val, cast(np.ndarray, y_pred), cast(np.ndarray, y_proba), training_time)
        
        self.models['RandomForest'] = model
        self.performance_metrics['RandomForest'] = metrics
        
        self.logger.info(f"éšæœºæ£®æ—è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {metrics.accuracy:.4f}, F1: {metrics.f1_score:.4f}")
        return metrics
        
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                          y_proba: np.ndarray, training_time: float) -> ModelPerformanceMetrics:
        """è®¡ç®—æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
        from sklearn.metrics import precision_score, recall_score
        
        return ModelPerformanceMetrics(
            accuracy=float(accuracy_score(y_true, y_pred)),
            precision=float(precision_score(y_true, y_pred, average='weighted', zero_division='warn')),
            recall=float(recall_score(y_true, y_pred, average='weighted', zero_division='warn')),
            f1_score=float(f1_score(y_true, y_pred, average='weighted', zero_division='warn')),
            auc_score=float(roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')),
            training_time=training_time
        )


class StatisticalABTestFramework:
    """ç»Ÿè®¡å­¦A/Bæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, significance_level: float = 0.05):
        self.significance_level = significance_level
        self.logger = logging.getLogger(f"{__name__}.ABTest")
        
    def mcnemar_test(self, model_a_name: str, model_b_name: str,
                    pred_a: np.ndarray, pred_b: np.ndarray, 
                    y_true: np.ndarray) -> ABTestResult:
        """éº¦å…‹å†…é©¬æ£€éªŒ - é…å¯¹æ ·æœ¬æ¯”è¾ƒ"""
        
        # æ„å»ºæ··æ·†çŸ©é˜µ
        correct_a = (pred_a == y_true).astype(int)
        correct_b = (pred_b == y_true).astype(int)
        
        # 2x2åˆ—è”è¡¨
        both_correct = np.sum((correct_a == 1) & (correct_b == 1))
        a_correct_b_wrong = np.sum((correct_a == 1) & (correct_b == 0))
        a_wrong_b_correct = np.sum((correct_a == 0) & (correct_b == 1))
        both_wrong = np.sum((correct_a == 0) & (correct_b == 0))
        
        self.logger.info(f"æ··æ·†çŸ©é˜µåˆ†æ:")
        self.logger.info(f"  ä¸¤è€…éƒ½æ­£ç¡®: {both_correct}")
        self.logger.info(f"  Aæ­£ç¡®Bé”™è¯¯: {a_correct_b_wrong}")
        self.logger.info(f"  Aé”™è¯¯Bæ­£ç¡®: {a_wrong_b_correct}")
        self.logger.info(f"  ä¸¤è€…éƒ½é”™è¯¯: {both_wrong}")
        
        # éº¦å…‹å†…é©¬ç»Ÿè®¡é‡è®¡ç®—
        discordant_pairs = a_correct_b_wrong + a_wrong_b_correct
        
        if discordant_pairs == 0:
            p_value = 1.0
            effect_size = 0.0
        else:
            # è¿ç»­æ€§æ ¡æ­£çš„éº¦å…‹å†…é©¬ç»Ÿè®¡é‡
            mcnemar_statistic = (abs(a_correct_b_wrong - a_wrong_b_correct) - 1) ** 2 / discordant_pairs
            p_value = 1 - stats.chi2.cdf(mcnemar_statistic, df=1)
            
            # æ•ˆåº”å¤§å° (Odds Ratio)
            if a_wrong_b_correct == 0:
                effect_size = float('inf') if a_correct_b_wrong > 0 else 0.0
            else:
                effect_size = a_correct_b_wrong / a_wrong_b_correct
        
        # å‡†ç¡®ç‡å·®å¼‚
        accuracy_a = np.mean(correct_a)
        accuracy_b = np.mean(correct_b)
        accuracy_diff = accuracy_a - accuracy_b
        
        # ç½®ä¿¡åŒºé—´ï¼ˆåŸºäºæ­£æ€è¿‘ä¼¼ï¼‰
        n = len(y_true)
        se_diff = np.sqrt((accuracy_a * (1 - accuracy_a) + accuracy_b * (1 - accuracy_b)) / n)
        margin_error = stats.norm.ppf(1 - self.significance_level / 2) * se_diff
        ci_lower = accuracy_diff - margin_error
        ci_upper = accuracy_diff + margin_error
        
        # åˆ¤æ–­æ˜¾è‘—æ€§å’Œè·èƒœè€…
        is_significant = bool(p_value < self.significance_level)
        
        if is_significant:
            if accuracy_a > accuracy_b:
                winner = model_a_name
            else:
                winner = model_b_name
        else:
            winner = "æ— æ˜¾è‘—å·®å¼‚"
            
        return ABTestResult(
            model_a_name=model_a_name,
            model_b_name=model_b_name,
            sample_size=len(y_true),
            p_value=float(p_value),
            effect_size=float(effect_size) if not np.isinf(effect_size) else 999.0,
            is_significant=is_significant,
            winner=winner,
            confidence_interval=(float(ci_lower), float(ci_upper))
        )
    
    def bootstrap_confidence_interval(self, model_a_pred: np.ndarray, model_b_pred: np.ndarray,
                                    y_true: np.ndarray, n_bootstrap: int = 1000) -> Tuple[float, float]:
        """è‡ªåŠ©æ³•ç½®ä¿¡åŒºé—´"""
        n_samples = len(y_true)
        differences = []
        
        for _ in range(n_bootstrap):
            # è‡ªåŠ©æŠ½æ ·
            indices = np.random.choice(n_samples, size=n_samples, replace=True)
            
            acc_a = accuracy_score(y_true[indices], model_a_pred[indices])
            acc_b = accuracy_score(y_true[indices], model_b_pred[indices])
            
            differences.append(acc_a - acc_b)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        alpha = self.significance_level
        ci_lower = float(np.percentile(differences, 100 * alpha / 2))
        ci_upper = float(np.percentile(differences, 100 * (1 - alpha / 2)))
        
        return ci_lower, ci_upper


class BusinessScenarioType(Enum):
    """ä¸šåŠ¡åœºæ™¯ç±»å‹æšä¸¾"""
    NORMAL_PERIOD = "normal_period"      # å¹³æ—¶æœŸ
    CAMPAIGN_PERIOD = "campaign_period"  # æ´»åŠ¨æœŸ
    HIGH_RISK_PERIOD = "high_risk_period" # é«˜é£é™©æœŸ
    MAINTENANCE_PERIOD = "maintenance_period" # ç»´æŠ¤æœŸ


@dataclass
class BusinessContext:
    """ä¸šåŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    scenario_type: BusinessScenarioType
    user_experience_priority: float  # ç”¨æˆ·ä½“éªŒä¼˜å…ˆçº§ [0,1]
    accuracy_requirement: float      # ç²¾åº¦è¦æ±‚ [0,1] 
    latency_requirement: float       # å»¶è¿Ÿè¦æ±‚ ms
    resource_constraint: float       # èµ„æºçº¦æŸ [0,1]
    traffic_volume_multiplier: float # æµé‡å€æ•°
    complex_feature_enabled: bool    # æ˜¯å¦å¯ç”¨å¤æ‚ç‰¹å¾å·¥ç¨‹


class IntelligentModelSelector:
    """ä¸šåŠ¡åœºæ™¯é©±åŠ¨çš„æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨
    
    æ ¸å¿ƒç†è®ºï¼šåŸºäºå¤šç›®æ ‡å†³ç­–ç†è®ºï¼Œæ ¹æ®ä¸šåŠ¡åœºæ™¯åŠ¨æ€é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    æ•°å­¦æ¨¡å‹ï¼šU(m) = Î£ w_i * s_i(m) + Î± * A(m, c)
    å…¶ä¸­ï¼šm-æ¨¡å‹, w_i-æƒé‡, s_i(m)-æ¨¡å‹å¾—åˆ†, A(m,c)-åœºæ™¯è°ƒæ•´å‡½æ•°
    """
    
    def __init__(self):
        self.model_profiles = self._initialize_model_profiles()
        self.selection_history = []
        self.logger = logging.getLogger(f"{__name__}.IntelligentSelector")
        
    def _initialize_model_profiles(self) -> Dict[str, Dict[str, float]]:
        """åˆå§‹åŒ–æ¨¡å‹ç‰¹æ€§æ¡£æ¡ˆ - åŸºäºç†è®ºåˆ†æå’Œå®éªŒæ•°æ®"""
        return {
            'LightGBM': {
                'speed_score': 0.95,          # é€Ÿåº¦è¯„åˆ†ï¼šç›´æ–¹å›¾ä¼˜åŒ–ç®—æ³•
                'accuracy_score': 0.85,       # ç²¾åº¦è¯„åˆ†ï¼šå¶å­ç”Ÿé•¿ç­–ç•¥
                'resource_efficiency': 0.90,  # èµ„æºæ•ˆç‡ï¼šå†…å­˜ä¼˜åŒ–
                'user_experience_impact': 0.95, # ç”¨æˆ·ä½“éªŒå½±å“ï¼šä½å»¶è¿Ÿ
                'scalability': 0.88,          # å¯æ‰©å±•æ€§ï¼šå¹¶è¡ŒåŒ–æ”¯æŒ
                'complexity_tolerance': 0.75   # å¤æ‚ç‰¹å¾å®¹å¿åº¦
            },
            'XGBoost': {
                'speed_score': 0.75,          # äºŒé˜¶æ¢¯åº¦è®¡ç®—å¼€é”€
                'accuracy_score': 0.92,       # äºŒé˜¶ä¼˜åŒ–ç²¾åº¦ä¼˜åŠ¿
                'resource_efficiency': 0.70,  # è®¡ç®—å¯†é›†å‹
                'user_experience_impact': 0.80, # ä¸­ç­‰å“åº”é€Ÿåº¦
                'scalability': 0.85,          # åˆ†å¸ƒå¼æ”¯æŒ
                'complexity_tolerance': 0.95   # æœ€é€‚åˆå¤æ‚ç‰¹å¾å·¥ç¨‹
            },
            'RandomForest': {
                'speed_score': 0.70,          # å¹¶è¡Œbaggingè®¡ç®—
                'accuracy_score': 0.88,       # æ–¹å·®å‡å°‘ç­–ç•¥
                'resource_efficiency': 0.75,  # ä¸­ç­‰èµ„æºæ¶ˆè€—
                'user_experience_impact': 0.85, # ç¨³å®šæ€§å¥½
                'scalability': 0.80,          # å¤©ç„¶å¹¶è¡ŒåŒ–
                'complexity_tolerance': 0.85   # ç‰¹å¾é€‰æ‹©èƒ½åŠ›
            },
            'Ensemble': {
                'speed_score': 0.60,           # å¤šæ¨¡å‹èåˆå¼€é”€
                'accuracy_score': 0.95,       # é›†æˆå­¦ä¹ ä¼˜åŠ¿
                'resource_efficiency': 0.50,  # é«˜èµ„æºæ¶ˆè€—
                'user_experience_impact': 0.70, # å¤æ‚å†³ç­–æµç¨‹
                'scalability': 0.65,          # å¤šæ¨¡å‹ç®¡ç†å¤æ‚åº¦
                'complexity_tolerance': 0.90   # å¤æ‚ç‰¹å¾èåˆèƒ½åŠ›
            }
        }
    
    def select_optimal_model(self, business_context: BusinessContext) -> Tuple[str, Dict[str, float]]:
        """æ ¹æ®ä¸šåŠ¡ä¸Šä¸‹æ–‡é€‰æ‹©æœ€ä¼˜æ¨¡å‹ - å¤šç›®æ ‡å†³ç­–ç®—æ³•"""
        
        # æ ¹æ®ä¸šåŠ¡åœºæ™¯è®¡ç®—æƒé‡åˆ†é…
        weights = self._calculate_scenario_weights(business_context)
        
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„ç»¼åˆå¾—åˆ†
        model_scores = {}
        
        for model_name, profile in self.model_profiles.items():
            # å¤šç›®æ ‡åŠ æƒå¾—åˆ†å…¬å¼ï¼šU(m) = Î£ w_i * s_i(m)
            total_score = (
                weights['speed'] * profile['speed_score'] +
                weights['accuracy'] * profile['accuracy_score'] +
                weights['resource'] * profile['resource_efficiency'] +
                weights['user_experience'] * profile['user_experience_impact'] +
                weights['scalability'] * profile['scalability'] +
                weights['complexity'] * profile['complexity_tolerance']
            )
            
            # ä¸šåŠ¡åœºæ™¯ç‰¹å®šè°ƒæ•´å‡½æ•° A(m, c)
            total_score = self._apply_scenario_adjustments(
                total_score, model_name, business_context
            )
            
            model_scores[model_name] = total_score
        
        # é€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼ˆæœ€å¤§åŒ–æ•ˆç”¨å‡½æ•°ï¼‰
        optimal_model = max(model_scores.items(), key=lambda x: x[1])
        
        # è®°å½•å†³ç­–å†å²ç”¨äºå­¦ä¹ ä¼˜åŒ–
        selection_record = {
            'timestamp': time.time(),
            'business_context': business_context,
            'weights': weights,
            'model_scores': model_scores,
            'selected_model': optimal_model[0],
            'selection_confidence': optimal_model[1]
        }
        self.selection_history.append(selection_record)
        
        self.logger.info(
            f"ä¸šåŠ¡åœºæ™¯: {business_context.scenario_type.value}, "
            f"é€‰æ‹©æ¨¡å‹: {optimal_model[0]}, "
            f"ç½®ä¿¡åº¦: {optimal_model[1]:.4f}"
        )
        
        return optimal_model[0], model_scores
    
    def _calculate_scenario_weights(self, context: BusinessContext) -> Dict[str, float]:
        """æ ¹æ®ä¸šåŠ¡åœºæ™¯è®¡ç®—æƒé‡åˆ†é… - åŸºäºé£æ§ä¸“å®¶ç»éªŒ"""
        
        if context.scenario_type == BusinessScenarioType.CAMPAIGN_PERIOD:
            # æ´»åŠ¨æœŸç­–ç•¥ï¼šä¼˜å…ˆè€ƒè™‘ç”¨æˆ·ä½“éªŒå’Œå“åº”é€Ÿåº¦
            return {
                'speed': 0.35,           # é«˜æƒé‡ï¼šå¿«é€Ÿå“åº”æ˜¯å…³é”®
                'user_experience': 0.30, # é«˜æƒé‡ï¼šä¿éšœæ´»åŠ¨ä½“éªŒ
                'accuracy': 0.15,        # é€‚ä¸­æƒé‡ï¼šåŸºæœ¬ç²¾åº¦è¦æ±‚
                'resource': 0.10,        # ä½æƒé‡ï¼šçŸ­æœŸæˆæœ¬å¯æ¥å—
                'scalability': 0.08,     # ä½æƒé‡ï¼šçŸ­æœŸæ‰©å±•éœ€æ±‚
                'complexity': 0.02       # æœ€ä½æƒé‡ï¼šé¿å…å¤æ‚åŒ–
            }
        
        elif context.scenario_type == BusinessScenarioType.NORMAL_PERIOD:
            # å¹³æ—¶æœŸç­–ç•¥ï¼šå¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡ï¼Œæ”¯æŒå¤æ‚ç‰¹å¾å·¥ç¨‹
            return {
                'accuracy': 0.30,        # é«˜æƒé‡ï¼šç²¾åº¦ä¼˜å…ˆåŸåˆ™
                'complexity': 0.25,      # é«˜æƒé‡ï¼šæ”¯æŒå¤æ‚ç‰¹å¾å·¥ç¨‹
                'speed': 0.20,           # é€‚ä¸­æƒé‡ï¼šæ•ˆç‡è¦æ±‚
                'resource': 0.15,        # é€‚ä¸­æƒé‡ï¼šèµ„æºä¼˜åŒ–
                'user_experience': 0.08, # ä½æƒé‡ï¼šå¹³æ—¶ä½“éªŒè¦æ±‚ä½
                'scalability': 0.02      # æœ€ä½æƒé‡ï¼šæ‰©å±•éœ€æ±‚ä¸ç´§æ€¥
            }
            
        elif context.scenario_type == BusinessScenarioType.HIGH_RISK_PERIOD:
            # é«˜é£é™©æœŸç­–ç•¥ï¼šç²¾åº¦è‡³ä¸Šï¼Œå¤æ‚ç‰¹å¾å·¥ç¨‹å…¨åŠ›å¼€å¯
            return {
                'accuracy': 0.45,        # æœ€é«˜æƒé‡ï¼šç²¾åº¦æ˜¯ç”Ÿå‘½çº¿
                'complexity': 0.30,      # é«˜æƒé‡ï¼šå…¨åŠ›ç‰¹å¾å·¥ç¨‹
                'scalability': 0.10,     # é€‚ä¸­æƒé‡ï¼šåº”å¯¹æ”»å‡»æ‰©å±•
                'speed': 0.08,           # ä½æƒé‡ï¼šé€Ÿåº¦è®©ä½ç²¾åº¦
                'resource': 0.05,        # ä½æƒé‡ï¼šä¸è®¡æˆæœ¬ä»£ä»·
                'user_experience': 0.02  # æœ€ä½æƒé‡ï¼šå®‰å…¨ç¬¬ä¸€
            }
            
        else:  # MAINTENANCE_PERIOD
            # ç»´æŠ¤æœŸç­–ç•¥ï¼šèµ„æºæ•ˆç‡å’Œç¨³å®šæ€§ä¼˜å…ˆ
            return {
                'resource': 0.35,        # é«˜æƒé‡ï¼šèŠ‚çº¦èµ„æºæˆæœ¬
                'speed': 0.25,           # é«˜æƒé‡ï¼šç®€å•é«˜æ•ˆè¿è¡Œ
                'user_experience': 0.20, # é€‚ä¸­æƒé‡ï¼šç»´æŒåŸºæœ¬ä½“éªŒ
                'accuracy': 0.15,        # é€‚ä¸­æƒé‡ï¼šåŸºæœ¬ç²¾åº¦è¦æ±‚
                'scalability': 0.03,     # ä½æƒé‡ï¼šæ‰©å±•éœ€æ±‚ä½
                'complexity': 0.02       # æœ€ä½æƒé‡ï¼šé¿å…å¤æ‚åŒ–
            }
    
    def _apply_scenario_adjustments(self, base_score: float, model_name: str, 
                                  context: BusinessContext) -> float:
        """åº”ç”¨ä¸šåŠ¡åœºæ™¯ç‰¹å®šè°ƒæ•´å‡½æ•° A(m, c)"""
        
        adjusted_score = base_score
        
        # æ´»åŠ¨æœŸç‰¹åŒ–ä¼˜åŒ–ï¼šLightGBM åœ¨é«˜å¹¶å‘æƒ…å†µä¸‹çš„æ€§èƒ½ä¼˜åŠ¿
        if (context.scenario_type == BusinessScenarioType.CAMPAIGN_PERIOD and 
            model_name == 'LightGBM' and context.traffic_volume_multiplier > 2.0):
            adjusted_score *= 1.15  # 15% åŠ æˆï¼šé«˜å¹¶å‘ä¼˜åŠ¿
            
        # å¹³æ—¶æœŸå¤æ‚ç‰¹å¾å·¥ç¨‹ï¼šXGBoost çš„äºŒé˜¶ä¼˜åŒ–ä¼˜åŠ¿
        if (context.scenario_type == BusinessScenarioType.NORMAL_PERIOD and 
            model_name == 'XGBoost' and context.complex_feature_enabled):
            adjusted_score *= 1.20  # 20% åŠ æˆï¼šå¤æ‚ç‰¹å¾ä¼˜åŠ¿
            
        # é«˜é£é™©æœŸç²¾åº¦ä¼˜å…ˆï¼šé›†æˆæ¨¡å‹çš„ç²¾åº¦ä¼˜åŠ¿
        if (context.scenario_type == BusinessScenarioType.HIGH_RISK_PERIOD and 
            model_name == 'Ensemble' and context.accuracy_requirement > 0.8):
            adjusted_score *= 1.25  # 25% åŠ æˆï¼šé›†æˆç²¾åº¦ä¼˜åŠ¿
            
        # ç”¨æˆ·ä½“éªŒä¸¥æ ¼è¦æ±‚ä¸‹çš„åŠ¨æ€è°ƒæ•´
        if context.user_experience_priority > 0.8:
            if model_name in ['LightGBM', 'RandomForest']:
                adjusted_score *= 1.10  # å¿«é€Ÿæ¨¡å‹åŠ æˆ
            elif model_name == 'Ensemble':
                adjusted_score *= 0.85  # é›†æˆæ¨¡å‹å‡åˆ†
                
        return adjusted_score
    
    def get_business_recommendation(self, context: BusinessContext) -> str:
        """ç”Ÿæˆä¸šåŠ¡åœºæ™¯é©±åŠ¨çš„æ¨¡å‹é€‰æ‹©å»ºè®®"""
        
        optimal_model, model_scores = self.select_optimal_model(context)
        
        recommendation = f"\nğŸ¯ åŸºäºä¸šåŠ¡åœºæ™¯çš„æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç»“æœ:\n"
        recommendation += f"åœºæ™¯ç±»å‹: {context.scenario_type.value}\n"
        recommendation += f"æ¨èæ¨¡å‹: {optimal_model}\n"
        recommendation += f"é€‰æ‹©ç½®ä¿¡åº¦: {model_scores[optimal_model]:.4f}\n\n"
        
        # æ ¹æ®ä¸åŒåœºæ™¯æä¾›ç†è®ºè§£é‡Š
        if context.scenario_type == BusinessScenarioType.CAMPAIGN_PERIOD:
            if optimal_model == 'LightGBM':
                recommendation += "ğŸš€ æ´»åŠ¨æœŸé€‰æ‹©LightGBMçš„ç†è®ºä¾æ®:\n"
                recommendation += "â€¢ ç›´æ–¹å›¾ä¼˜åŒ–ç®—æ³•æä¾›æè‡´çš„æ¨ç†é€Ÿåº¦\n"
                recommendation += "â€¢ å¶å­ç”Ÿé•¿ç­–ç•¥åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹æ€§èƒ½ç¨³å®š\n"
                recommendation += "â€¢ å†…å­˜æ•ˆç‡é«˜ï¼Œé€‚åˆå¤§æµé‡å†²å‡»\n"
                recommendation += "â€¢ ä¼˜å…ˆç”¨æˆ·ä½“éªŒï¼Œå‡å°‘æ´»åŠ¨æœŸé—´çš„ç”¨æˆ·æµå¤±\n"
        
        elif context.scenario_type == BusinessScenarioType.NORMAL_PERIOD:
            if optimal_model == 'XGBoost':
                recommendation += "ğŸ”¬ å¹³æ—¶æœŸé€‰æ‹©XGBoostçš„ç†è®ºä¾æ®:\n"
                recommendation += "â€¢ äºŒé˜¶æ¢¯åº¦ä¼˜åŒ–ç®—æ³•æä¾›æœ€ä¼˜ç²¾åº¦\n"
                recommendation += "â€¢ å¼ºå¤§çš„æ­£åˆ™åŒ–æœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ\n"
                recommendation += "â€¢ å¯¹å¤æ‚ç‰¹å¾å·¥ç¨‹æ”¯æŒæœ€ä¼˜ï¼Œé€‚åˆæ·±åº¦åˆ†æ\n"
                recommendation += "â€¢ å¹³æ—¶æœŸæœ‰å……è¶³æ—¶é—´è¿›è¡Œç²¾ç»†ç‰¹å¾å·¥ç¨‹\n"
        
        elif context.scenario_type == BusinessScenarioType.HIGH_RISK_PERIOD:
            if optimal_model == 'Ensemble':
                recommendation += "ğŸ›¡ï¸ é«˜é£é™©æœŸé€‰æ‹©Ensembleæ¨¡å‹çš„ç†è®ºä¾æ®:\n"
                recommendation += "â€¢ é›†æˆå­¦ä¹ æä¾›æœ€é«˜çš„æ£€æµ‹ç²¾åº¦\n"
                recommendation += "â€¢ å¤šæ¨¡å‹èåˆé™ä½å•ä¸€æ¨¡å‹çš„é£é™©\n"
                recommendation += "â€¢ ä¸è®¡æˆæœ¬ä»£ä»·ï¼Œä»¥å®‰å…¨é˜²æŠ¤ä¸ºç¬¬ä¸€ä¼˜å…ˆçº§\n"
                recommendation += "â€¢ å¤æ‚ç‰¹å¾ç»„åˆæä¾›æœ€å…¨é¢çš„å¨èƒè¯†åˆ«\n"
        
        # æ·»åŠ ä¸šåŠ¡æŒ‡æ ‡é¢„æœŸ
        recommendation += f"\nğŸ“Š é¢„æœŸä¸šåŠ¡æŒ‡æ ‡è¡¨ç°:\n"
        recommendation += f"ç²¾åº¦è¡¨ç°: {self.model_profiles[optimal_model]['accuracy_score']*100:.1f}%\n"
        recommendation += f"é€Ÿåº¦è¡¨ç°: {self.model_profiles[optimal_model]['speed_score']*100:.1f}%\n"
        recommendation += f"èµ„æºæ•ˆç‡: {self.model_profiles[optimal_model]['resource_efficiency']*100:.1f}%\n"
        recommendation += f"ç”¨æˆ·ä½“éªŒ: {self.model_profiles[optimal_model]['user_experience_impact']*100:.1f}%\n"
        
        return recommendation


class NetworkFlowAnalyzer:
    """ç½‘ç»œæµé‡åˆ†æå™¨ - é›†æˆVPNæ£€æµ‹ç®—æ³•"""
    
    def __init__(self):
        self.ike_esp_ports = [500, 4500]  # IPsec IKE/ESP
        self.openvpn_ports = [1194]       # OpenVPN
        self.wireguard_ports = [51820]    # WireGuard
        self.ddos_threshold = 1000        # DDoSæ£€æµ‹é˜ˆå€¼
        self.sliding_window_size = 60     # æ»‘åŠ¨çª—å£å¤§å°(ç§’)
        self.packet_history = deque(maxlen=10000)
        self.flow_cache = {}
        self.baseline_distributions = None
        self.logger = logging.getLogger(f"{__name__}.NetworkFlow")
        
    def generate_network_packets(self, attack_type: AttackType, count: int = 100) -> List[NetworkPacket]:
        """ç”Ÿæˆç½‘ç»œæ•°æ®åŒ… - æ¨¡æ‹Ÿä¸åŒçš„æ”»å‡»ç±»å‹"""
        packets = []
        base_time = time.time()
        
        np.random.seed(42 + attack_type.value)
        
        for i in range(count):
            if attack_type == AttackType.VPN_TUNNEL:
                # VPNéš§é“æµé‡ç‰¹å¾
                src_port = np.random.choice(self.openvpn_ports + self.ike_esp_ports)
                dst_port = np.random.choice(self.openvpn_ports + self.ike_esp_ports)
                size = int(np.random.normal(800, 100))  # æ›´è§„å¾‹çš„åŒ…å¤§å°
                direction = np.random.choice(['up', 'down'], p=[0.45, 0.55])
                time_interval = np.random.exponential(0.1)  # æ›´è§„å¾‹çš„æ—¶é—´é—´éš”
                protocol = "UDP"
                
            elif attack_type == AttackType.DDOS:
                # DDoSæ”»å‡»ç‰¹å¾
                src_port = np.random.randint(1024, 65535)
                dst_port = np.random.choice([80, 443, 53])
                size = int(np.random.choice([64, 128, 256]))  # å°åŒ…æ”»å‡»
                direction = 'up'  # ä¸»è¦æ˜¯ä¸Šè¡Œæµé‡
                time_interval = np.random.exponential(0.01)  # éå¸¸å¿«çš„è¯·æ±‚
                protocol = np.random.choice(["TCP", "UDP"])
                
            elif attack_type == AttackType.NETWORK_ANOMALY:
                # ç½‘ç»œå¼‚å¸¸æµé‡
                src_port = np.random.randint(1024, 65535)
                dst_port = np.random.randint(1024, 65535)  # éšæœºç«¯å£
                size = int(np.random.uniform(100, 1500))  # ä¸è§„åˆ™åŒ…å¤§å°
                direction = np.random.choice(['up', 'down'])
                time_interval = np.random.uniform(0.001, 1.0)  # ä¸è§„åˆ™æ—¶é—´é—´éš”
                protocol = np.random.choice(["TCP", "UDP", "ICMP"])
                
            else:  # NORMAL
                # æ­£å¸¸æµé‡ç‰¹å¾
                src_port = np.random.randint(1024, 65535)
                dst_port = np.random.choice([80, 443, 53, 8080])
                size = int(np.random.choice([64, 128, 256, 512, 1024, 1500]))
                direction = np.random.choice(['up', 'down'])
                time_interval = np.random.exponential(np.random.uniform(0.05, 0.5))
                protocol = np.random.choice(["TCP", "UDP"])
                
            packet = NetworkPacket(
                timestamp=base_time + i * time_interval,
                src_ip=f"192.168.1.{np.random.randint(1, 254)}",
                dst_ip=f"8.8.8.{np.random.randint(1, 254)}",
                src_port=src_port,
                dst_port=dst_port,
                protocol=protocol,
                size=max(64, min(1500, size)),
                direction=direction,
                payload_size=max(20, size - 40)
            )
            packets.append(packet)
            
        return packets
    
    def detect_vpn_tunnel(self, packets: List[NetworkPacket]) -> NetworkDetectionResult:
        """å››é˜¶æ®µçº§è”VPNæ£€æµ‹ç®—æ³•"""
        if not packets:
            return NetworkDetectionResult(
                flow_id="empty_flow",
                is_threat=False,
                threat_type="NORMAL",
                confidence=0.9,
                detection_stage="EmptyFlow",
                features={},
                timestamp=time.time()
            )
            
        flow_id = f"{packets[0].src_ip}:{packets[0].dst_ip}"
        
        # é˜¶æ®µA: è§„åˆ™é¢„ç­›
        protocol_indicators = self._check_protocol_indicators(packets)
        if not any(protocol_indicators.values()):
            return NetworkDetectionResult(
                flow_id=flow_id,
                is_threat=False,
                threat_type="NORMAL",
                confidence=0.95,
                detection_stage="RulePreFilter",
                features=protocol_indicators,
                timestamp=time.time()
            )
        
        # æå–ç½‘ç»œç‰¹å¾
        features = self._extract_network_features(packets)
        
        # é˜¶æ®µB: ç›¸å¯¹ç†µè¿‡æ»¤
        if not self._relative_entropy_filter(features):
            return NetworkDetectionResult(
                flow_id=flow_id,
                is_threat=False,
                threat_type="NORMAL",
                confidence=0.8,
                detection_stage="RelativeEntropyFilter",
                features=features,
                timestamp=time.time()
            )
        
        # é˜¶æ®µC: åºåˆ—æ¨¡å‹ç²¾åˆ¤
        sequence_score = self._cnn_lstm_predict(packets)
        
        # é˜¶æ®µD: å¤šçª—èåˆï¼ˆç®€åŒ–ç‰ˆï¼‰
        is_vpn = sequence_score > 0.6
        confidence = sequence_score if is_vpn else 1.0 - sequence_score
        threat_type = "VPN_TUNNEL" if is_vpn else "NORMAL"
        
        return NetworkDetectionResult(
            flow_id=flow_id,
            is_threat=is_vpn,
            threat_type=threat_type,
            confidence=confidence,
            detection_stage="SequenceModel",
            features=features,
            timestamp=time.time()
        )
    
    def _check_protocol_indicators(self, packets: List[NetworkPacket]) -> Dict[str, bool]:
        """æ£€æŸ¥åè®®æŒ‡ç¤ºå™¨"""
        indicators = {
            'ike_esp_detected': False,
            'dtls_tls_tunnel': False,
            'vpn_port_detected': False
        }
        
        for packet in packets:
            # æ£€æŸ¥IPsec IKE/ESP
            if packet.dst_port in self.ike_esp_ports or packet.src_port in self.ike_esp_ports:
                indicators['ike_esp_detected'] = True
                
            # æ£€æŸ¥VPNå¸¸ç”¨ç«¯å£
            vpn_ports = self.ike_esp_ports + self.openvpn_ports + self.wireguard_ports
            if packet.dst_port in vpn_ports or packet.src_port in vpn_ports:
                indicators['vpn_port_detected'] = True
                
            # æ£€æŸ¥DTLS/TLSéš§é“
            if packet.dst_port == 443 and packet.protocol == 'UDP':
                indicators['dtls_tls_tunnel'] = True
                
        return indicators
    
    def _extract_network_features(self, packets: List[NetworkPacket]) -> Dict[str, Any]:
        """æå–ç½‘ç»œæµé‡ç‰¹å¾"""
        if not packets:
            return {}
            
        # åˆ†ç¦»ä¸Šè¡Œå’Œä¸‹è¡Œæµé‡
        up_packets = [p for p in packets if p.direction == 'up']
        down_packets = [p for p in packets if p.direction == 'down']
        
        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
        packet_sizes = [p.size for p in packets]
        
        # æ—¶é—´é—´éš”ç‰¹å¾
        iats = []
        if len(packets) > 1:
            iats = [packets[i].timestamp - packets[i-1].timestamp for i in range(1, len(packets))]
        
        # ç«¯å£åˆ†æ
        dst_ports = [p.dst_port for p in packets]
        vpn_ports = self.ike_esp_ports + self.openvpn_ports + self.wireguard_ports
        vpn_port_count = sum(1 for port in dst_ports if port in vpn_ports)
        
        features = {
            'packet_count': len(packets),
            'total_bytes': sum(p.size for p in packets),
            'direction_ratio': len(up_packets) / len(packets) if packets else 0,
            'avg_packet_size': np.mean(packet_sizes) if packet_sizes else 0,
            'packet_size_std': np.std(packet_sizes) if packet_sizes else 0,
            'avg_iat': np.mean(iats) if iats else 0,
            'iat_std': np.std(iats) if iats else 0,
            'vpn_port_ratio': vpn_port_count / len(dst_ports) if dst_ports else 0,
            'flow_duration': packets[-1].timestamp - packets[0].timestamp if len(packets) > 1 else 0,
            'packets_per_second': len(packets) / (packets[-1].timestamp - packets[0].timestamp + 1e-6) if len(packets) > 1 else 0
        }
        
        return features
    
    def _relative_entropy_filter(self, features: Dict[str, Any], threshold: float = 0.1) -> bool:
        """ç›¸å¯¹ç†µè¿‡æ»¤ - ç®€åŒ–ç‰ˆ"""
        # å¦‚æœæ²¡æœ‰åŸºçº¿ï¼Œé»˜è®¤é€šè¿‡
        if self.baseline_distributions is None:
            return True
            
        # åŸºäºç»Ÿè®¡ç‰¹å¾çš„ç®€å•åˆ¤æ–­
        if features.get('vpn_port_ratio', 0) > 0.5:  # VPNç«¯å£æ¯”ä¾‹é«˜
            return True
        if features.get('avg_iat', 0) < 0.2 and features.get('iat_std', 0) < 0.1:  # é«˜è§„å¾‹æ€§
            return True
            
        return False
    
    def _cnn_lstm_predict(self, packets: List[NetworkPacket]) -> float:
        """æ¨¡æ‹Ÿ1D-CNN + LSTMé¢„æµ‹"""
        if not packets:
            return 0.5
            
        packet_sizes = np.array([p.size for p in packets])
        directions = np.array([1 if p.direction == 'up' else 0 for p in packets])
        
        # æ¨¡æ‹Ÿ1D-CNNç‰¹å¾æå–
        cnn_kernel = np.array([0.1, 0.3, 0.3, 0.3, 0.1])
        if len(packet_sizes) >= len(cnn_kernel):
            cnn_features = np.convolve(packet_sizes, cnn_kernel, mode='valid')
        else:
            cnn_features = packet_sizes
            
        # æ¨¡æ‹ŸLSTMæ—¶åºå»ºæ¨¡
        lstm_output = np.mean(cnn_features) if len(cnn_features) > 0 else np.mean(packet_sizes)
        
        # ç»“åˆæ–¹å‘ä¿¡æ¯
        direction_score = np.mean(directions) if len(directions) > 0 else 0.5
        
        # ç»¼åˆå¾—åˆ†
        final_score = (lstm_output / 1500 + direction_score) / 2
        return float(np.clip(final_score, 0, 1))


class EnhancedRiskDetector:
    """å¢å¼ºå‹é£æ§æ£€æµ‹å™¨ - èåˆä¸šåŠ¡é£æ§å’Œç½‘ç»œå¨èƒæ£€æµ‹"""
    
    def __init__(self):
        # ä¸šåŠ¡é£æ§ç»„ä»¶
        self.gradient_comparator = GradientBoostingComparator()
        self.intelligent_selector = IntelligentModelSelector()
        
        # ç½‘ç»œå¨èƒæ£€æµ‹ç»„ä»¶
        self.network_analyzer = NetworkFlowAnalyzer()
        
        # èåˆå‚æ•°
        self.business_weight = 0.7  # ä¸šåŠ¡å±‚æƒé‡
        self.network_weight = 0.3   # ç½‘ç»œå±‚æƒé‡
        
        self.logger = logging.getLogger(f"{__name__}.EnhancedRisk")
        
    def comprehensive_risk_assessment(self, business_features: np.ndarray, 
                                    network_packets: List[NetworkPacket],
                                    business_context: BusinessContext) -> Dict[str, Any]:
        """ç»¼åˆé£é™©è¯„ä¼° - èåˆä¸šåŠ¡å’Œç½‘ç»œå±‚å¨èƒ"""
        
        assessment_start = time.time()
        
        # 1. ä¸šåŠ¡åœºæ™¯é©±åŠ¨çš„æ¨¡å‹é€‰æ‹©
        optimal_model, model_scores = self.intelligent_selector.select_optimal_model(business_context)
        self.logger.info(f"é€‰æ‹©æ¨¡å‹: {optimal_model}, åœºæ™¯: {business_context.scenario_type.value}")
        
        # 2. ä¸šåŠ¡å±‚é£é™©æ£€æµ‹
        business_risk_score = 0.5  # é»˜è®¤å€¼ï¼Œå®é™…åº”è¯¥ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹
        business_threat_type = "NORMAL"
        
        # æ¨¡æ‹Ÿä¸šåŠ¡å±‚é£é™©è¯„åˆ†
        if len(business_features) > 0:
            # ç®€åŒ–çš„ä¸šåŠ¡é£é™©è¯„åˆ†ç®—æ³•
            feature_sum = np.sum(business_features)
            if feature_sum > 50:  # é«˜é£é™©é˜ˆå€¼
                business_risk_score = 0.8
                business_threat_type = "HIGH_BUSINESS_RISK"
            elif feature_sum > 30:  # ä¸­é£é™©é˜ˆå€¼
                business_risk_score = 0.6
                business_threat_type = "MEDIUM_BUSINESS_RISK"
            else:
                business_risk_score = 0.3
                business_threat_type = "LOW_BUSINESS_RISK"
        
        # 3. ç½‘ç»œå±‚å¨èƒæ£€æµ‹
        network_detection = self.network_analyzer.detect_vpn_tunnel(network_packets)
        network_risk_score = network_detection.confidence if network_detection.is_threat else 0.3
        
        # 4. å¤šå±‚æ¬¡é£é™©èåˆç®—æ³•
        # ä½¿ç”¨åŠ æƒèåˆ: R_total = w1 * R_business + w2 * R_network + Î± * I(business, network)
        base_risk = (self.business_weight * business_risk_score + 
                    self.network_weight * network_risk_score)
        
        # äº¤äº’é¡¹: å¦‚æœåŒæ—¶æ£€æµ‹åˆ°ä¸šåŠ¡å’Œç½‘ç»œå¨èƒï¼Œå¢åŠ é£é™©è¯„åˆ†
        interaction_boost = 0.0
        if business_risk_score > 0.6 and network_risk_score > 0.6:
            interaction_boost = 0.2  # 20% çš„ååŒå¨èƒåŠ æˆ
            
        total_risk_score = min(1.0, base_risk + interaction_boost)
        
        # 5. å¨èƒç±»å‹ç»¼åˆåˆ¤æ–­
        if total_risk_score > 0.8:
            final_threat_level = "HIGH"
        elif total_risk_score > 0.6:
            final_threat_level = "MEDIUM"
        else:
            final_threat_level = "LOW"
            
        # 6. ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
        assessment_result = {
            'total_risk_score': float(total_risk_score),
            'threat_level': final_threat_level,
            'business_assessment': {
                'risk_score': float(business_risk_score),
                'threat_type': business_threat_type,
                'selected_model': optimal_model,
                'model_scores': model_scores
            },
            'network_assessment': {
                'risk_score': float(network_risk_score),
                'threat_type': network_detection.threat_type,
                'detection_stage': network_detection.detection_stage,
                'flow_id': network_detection.flow_id
            },
            'fusion_analysis': {
                'business_weight': self.business_weight,
                'network_weight': self.network_weight,
                'interaction_boost': interaction_boost,
                'assessment_time': time.time() - assessment_start
            },
            'recommendations': self._generate_recommendations(
                business_risk_score, network_risk_score, total_risk_score, business_context
            )
        }
        
        return assessment_result
    
    def _generate_recommendations(self, business_risk: float, network_risk: float, 
                                total_risk: float, context: BusinessContext) -> List[str]:
        """ç”Ÿæˆé£æ§å»ºè®®"""
        recommendations = []
        
        if total_risk > 0.8:
            recommendations.append("ğŸš¨ é«˜é£é™©è­¦å‘Šï¼šå»ºè®®ç«‹å³é˜»æ–­å¹¶äººå·¥å®¡æ ¸")
            if business_risk > 0.7:
                recommendations.append("ğŸ“Š ä¸šåŠ¡å±‚å¨èƒï¼šåŠ å¼ºè®¢å•/æ”¯ä»˜éªŒè¯")
            if network_risk > 0.7:
                recommendations.append("ğŸŒ ç½‘ç»œå±‚å¨èƒï¼šæ£€æŸ¥VPN/ä»£ç†å·¥å…·ä½¿ç”¨")
                
        elif total_risk > 0.6:
            recommendations.append("âš ï¸ ä¸­ç­‰é£é™©ï¼šå¢å¼ºç›‘æ§å’ŒéªŒè¯")
            if context.scenario_type == BusinessScenarioType.CAMPAIGN_PERIOD:
                recommendations.append("ğŸ¯ æ´»åŠ¨æœŸå»ºè®®ï¼šå¹³è¡¡ç”¨æˆ·ä½“éªŒä¸å®‰å…¨éªŒè¯")
            else:
                recommendations.append("ğŸ” å»ºè®®å¯ç”¨å¤æ‚ç‰¹å¾å·¥ç¨‹è¿›è¡Œæ·±åº¦åˆ†æ")
                
        else:
            recommendations.append("âœ… ä½é£é™©ï¼šæ­£å¸¸å¤„ç†")
            
        return recommendations


def demo_integrated_detection():
    """æ¼”ç¤ºé›†æˆçš„å¤šå±‚æ¬¡é£æ§æ£€æµ‹ç³»ç»Ÿ"""
    print("ğŸš€ å¤šå±‚æ¬¡é£æ§æ£€æµ‹ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 70)
    
    # åˆ›å»ºå¢å¼ºå‹é£æ§æ£€æµ‹å™¨
    enhanced_detector = EnhancedRiskDetector()
    
    # åˆ›å»ºç½‘ç»œæµé‡åˆ†æå™¨
    network_analyzer = NetworkFlowAnalyzer()
    
    print("ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    
    # 1. ç”Ÿæˆä¸åŒç±»å‹çš„ç½‘ç»œæ•°æ®åŒ…
    test_scenarios = [
        (AttackType.NORMAL, "æ­£å¸¸æµé‡"),
        (AttackType.VPN_TUNNEL, "VPNéš§é“"),
        (AttackType.DDOS, "DDoSæ”»å‡»"),
        (AttackType.NETWORK_ANOMALY, "ç½‘ç»œå¼‚å¸¸")
    ]
    
    # 2. æ¨¡æ‹Ÿä¸šåŠ¡ç‰¹å¾æ•°æ®
    business_features_normal = np.array([15, 12, 500, 2, 0.8, 0.13, 33, 12, 3, 0.5, 0.3, 50, 0.8, 0.9, 0.1, 0.2, 1.5, 0.1])
    business_features_risky = np.array([80, 2, 300, 25, 0.025, 0.31, 3.75, 12, 3, 0.5, 0.7, 200, 0.2, 0.2, 0.5, 0.7, 5.0, 0.3])
    
    # 3. åˆ›å»ºä¸šåŠ¡ä¸Šä¸‹æ–‡
    campaign_context = BusinessContext(
        scenario_type=BusinessScenarioType.CAMPAIGN_PERIOD,
        user_experience_priority=0.9,
        accuracy_requirement=0.7,
        latency_requirement=50,
        resource_constraint=0.3,
        traffic_volume_multiplier=3.0,
        complex_feature_enabled=False
    )
    
    print("\nğŸ” æ‰§è¡Œç½‘ç»œå¨èƒæ£€æµ‹æµ‹è¯•:")
    print("-" * 50)
    
    # åˆå§‹åŒ–comprehensive_resultå˜é‡ï¼Œé¿å…æœªç»‘å®šé”™è¯¯
    comprehensive_result = {
        'total_risk_score': 0.0,
        'threat_level': 'LOW',
        'business_assessment': {'risk_score': 0.0, 'threat_type': 'NORMAL'},
        'network_assessment': {'risk_score': 0.0, 'threat_type': 'NORMAL'},
        'fusion_analysis': {},
        'recommendations': ['æ— å¨èƒæ£€æµ‹']
    }
    
    for attack_type, description in test_scenarios:
        print(f"\næµ‹è¯•åœºæ™¯: {description}")
        
        # ç”Ÿæˆç½‘ç»œæ•°æ®åŒ…
        packets = network_analyzer.generate_network_packets(attack_type, count=50)
        print(f"  ç”Ÿæˆæ•°æ®åŒ…æ•°é‡: {len(packets)}")
        
        # VPNæ£€æµ‹
        vpn_result = network_analyzer.detect_vpn_tunnel(packets)
        print(f"  VPNæ£€æµ‹ç»“æœ: {vpn_result.threat_type} (ç½®ä¿¡åº¦: {vpn_result.confidence:.3f})")
        print(f"  æ£€æµ‹é˜¶æ®µ: {vpn_result.detection_stage}")
        
        # ç»¼åˆé£é™©è¯„ä¼°
        if attack_type in [AttackType.VPN_TUNNEL, AttackType.DDOS]:
            business_features = business_features_risky
        else:
            business_features = business_features_normal
            
        comprehensive_result = enhanced_detector.comprehensive_risk_assessment(
            business_features, packets, campaign_context
        )
        
        print(f"  ç»¼åˆé£é™©è¯„åˆ†: {comprehensive_result['total_risk_score']:.3f}")
        print(f"  å¨èƒç­‰çº§: {comprehensive_result['threat_level']}")
        print(f"  ä¸šåŠ¡é£é™©: {comprehensive_result['business_assessment']['risk_score']:.3f}")
        print(f"  ç½‘ç»œé£é™©: {comprehensive_result['network_assessment']['risk_score']:.3f}")
        print(f"  æ¨èæªæ–½: {comprehensive_result['recommendations'][0]}")
    
    print("\nğŸ¯ ç†è®ºæ¡†æ¶æ€»ç»“:")
    print("-" * 50)
    print("1. å››é˜¶æ®µçº§è”VPNæ£€æµ‹: è§„åˆ™é¢„ç­› â†’ ç›¸å¯¹ç†µè¿‡æ»¤ â†’ åºåˆ—æ¨¡å‹ç²¾åˆ¤ â†’ å¤šçª—èåˆ")
    print("2. ä¸šåŠ¡åœºæ™¯é©±åŠ¨é€‰æ‹©: æ ¹æ®æ´»åŠ¨æœŸ/å¹³æ—¶æœŸ/é«˜é£é™©æœŸæ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ¨¡å‹")
    print("3. å¤šå±‚æ¬¡é£é™©èåˆ: R_total = w1*R_business + w2*R_network + Î±*I(business,network)")
    print("4. è‡ªé€‚åº”å¨èƒå“åº”: åŸºäºå¨èƒç­‰çº§å’Œä¸šåŠ¡åœºæ™¯ç”Ÿæˆå·®å¼‚åŒ–å¤„ç†å»ºè®®")
    
    return comprehensive_result


def main() -> Dict[str, Any]:
    """ä¸»æ¼”ç¤ºå‡½æ•° - å¤šå±‚æ¬¡é£æ§ç³»ç»Ÿå®Œæ•´å±•ç¤º"""
    
    # 1. ç”Ÿæˆæ•°æ®é›†å¹¶æ·»åŠ ç±»å‹æ³¨è§£
    print("ğŸ“Š ç”Ÿæˆå¹³è¡¡çš„é£æ§æ•°æ®é›†...")
    X, y = AdvancedDataGenerator.generate_balanced_dataset(total_samples=3000)
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    X: np.ndarray = cast(np.ndarray, X)
    y: np.ndarray = cast(np.ndarray, y)
    
    print(f"æ•°æ®é›†è§„æ¨¡: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    
    # 2. æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled: np.ndarray = cast(np.ndarray, scaler.fit_transform(X))
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # ç¡®ä¿è¿”å›å€¼æ˜¯æ­£ç¡®çš„numpyæ•°ç»„ç±»å‹
    X_train = cast(np.ndarray, X_train)
    X_test = cast(np.ndarray, X_test)
    y_train = cast(np.ndarray, y_train)
    y_test = cast(np.ndarray, y_test)
    
    # è¿›ä¸€æ­¥åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train_sub, X_val, y_train_sub, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æ­£ç¡®çš„numpyæ•°ç»„ç±»å‹
    X_train_sub = cast(np.ndarray, X_train_sub)
    X_val = cast(np.ndarray, X_val)
    y_train_sub = cast(np.ndarray, y_train_sub)
    y_val = cast(np.ndarray, y_val)
    
    # è®­ç»ƒæ¨¡å‹
    comparator = GradientBoostingComparator()
    xgb_metrics = comparator.train_xgboost(X_train_sub, y_train_sub, X_val, y_val)
    lgb_metrics = comparator.train_lightgbm(X_train_sub, y_train_sub, X_val, y_val)
    rf_metrics = comparator.train_random_forest(X_train_sub, y_train_sub, X_val, y_val)
    
    # 3. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
    print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print("-" * 70)
    
    available_models = []
    for model_name, metrics in comparator.performance_metrics.items():
        if metrics is not None:
            available_models.append(model_name)
            print(f"{model_name:12} | å‡†ç¡®ç‡: {metrics.accuracy:.4f} | F1: {metrics.f1_score:.4f} | "
                  f"AUC: {metrics.auc_score:.4f} | è®­ç»ƒæ—¶é—´: {metrics.training_time:.2f}s")
    
    # 4. A/Bæµ‹è¯•
    print("\nğŸ§ª æ‰§è¡ŒA/Bæµ‹è¯•ç»Ÿè®¡åˆ†æ...")
    ab_framework = StatisticalABTestFramework()
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
    test_predictions = {}
    for model_name in available_models:
        model = comparator.models[model_name]
        test_predictions[model_name] = model.predict(X_test)
    
    # æ‰§è¡Œä¸¤ä¸¤A/Bæµ‹è¯•
    ab_results = []
    for i, model_a in enumerate(available_models):
        for j, model_b in enumerate(available_models):
            if i < j:  # é¿å…é‡å¤æ¯”è¾ƒ
                result = ab_framework.mcnemar_test(
                    model_a, model_b,
                    test_predictions[model_a],
                    test_predictions[model_b],
                    y_test
                )
                ab_results.append(result)
                
                print(f"\n{model_a} vs {model_b}:")
                print(f"  p-value: {result.p_value:.6f}")
                print(f"  æ•ˆåº”å¤§å°: {result.effect_size:.4f}")
                print(f"  ç»Ÿè®¡æ˜¾è‘—æ€§: {'æ˜¯' if result.is_significant else 'å¦'}")
                print(f"  è·èƒœè€…: {result.winner}")
                print(f"  ç½®ä¿¡åŒºé—´: [{result.confidence_interval[0]:.4f}, {result.confidence_interval[1]:.4f}]")
    
    # 5. ä¸šåŠ¡åœºæ™¯é©±åŠ¨çš„æ™ºèƒ½æ¨¡å‹é€‰æ‹©æ¼”ç¤º
    print("\nğŸ¤– ä¸šåŠ¡åœºæ™¯é©±åŠ¨çš„æ™ºèƒ½æ¨¡å‹é€‰æ‹©æ¼”ç¤º:")
    print("=" * 70)
    
    # åˆå§‹åŒ–æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨
    intelligent_selector = IntelligentModelSelector()
    
    # æ¨¡æ‹Ÿä¸åŒä¸šåŠ¡åœºæ™¯
    business_scenarios = [
        # æ´»åŠ¨æœŸåœºæ™¯ï¼šä¼˜å…ˆç”¨æˆ·ä½“éªŒå’Œé€Ÿåº¦
        BusinessContext(
            scenario_type=BusinessScenarioType.CAMPAIGN_PERIOD,
            user_experience_priority=0.9,  # é«˜ç”¨æˆ·ä½“éªŒè¦æ±‚
            accuracy_requirement=0.7,      # ä¸­ç­‰ç²¾åº¦è¦æ±‚
            latency_requirement=50,        # ä½å»¶è¿Ÿè¦æ±‚
            resource_constraint=0.3,       # ä½èµ„æºçº¦æŸ
            traffic_volume_multiplier=3.0, # 3å€æµé‡
            complex_feature_enabled=False  # ä¸å¯ç”¨å¤æ‚ç‰¹å¾
        ),
        
        # å¹³æ—¶æœŸåœºæ™¯ï¼šå¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡ï¼Œæ”¯æŒå¤æ‚ç‰¹å¾
        BusinessContext(
            scenario_type=BusinessScenarioType.NORMAL_PERIOD,
            user_experience_priority=0.6,  # ä¸­ç­‰ä½“éªŒè¦æ±‚
            accuracy_requirement=0.85,     # é«˜ç²¾åº¦è¦æ±‚
            latency_requirement=100,       # ä¸­ç­‰å»¶è¿Ÿè¦æ±‚
            resource_constraint=0.6,       # ä¸­ç­‰èµ„æºçº¦æŸ
            traffic_volume_multiplier=1.0, # æ­£å¸¸æµé‡
            complex_feature_enabled=True   # å¯ç”¨å¤æ‚ç‰¹å¾å·¥ç¨‹
        ),
        
        # é«˜é£é™©æœŸåœºæ™¯ï¼šç²¾åº¦è‡³ä¸Š
        BusinessContext(
            scenario_type=BusinessScenarioType.HIGH_RISK_PERIOD,
            user_experience_priority=0.3,  # ä½ä½“éªŒè¦æ±‚ï¼ˆå®‰å…¨ç¬¬ä¸€ï¼‰
            accuracy_requirement=0.95,     # æé«˜ç²¾åº¦è¦æ±‚
            latency_requirement=200,       # å…è®¸è¾ƒé«˜å»¶è¿Ÿ
            resource_constraint=0.2,       # ä½èµ„æºçº¦æŸï¼ˆä¸è®¡æˆæœ¬ï¼‰
            traffic_volume_multiplier=1.5, # æ”»å‡»æµé‡
            complex_feature_enabled=True   # å…¨åŠ›ç‰¹å¾å·¥ç¨‹
        )
    ]
    
    # å¯¹æ¯ä¸ªä¸šåŠ¡åœºæ™¯è¿›è¡Œæ¨¡å‹é€‰æ‹©æ¼”ç¤º
    for i, context in enumerate(business_scenarios, 1):
        print(f"\nğŸ’¼ ä¸šåŠ¡åœºæ™¯ {i}: {context.scenario_type.value}")
        print("-" * 50)
        
        # è·å–æ™ºèƒ½æ¨¡å‹é€‰æ‹©å»ºè®®
        recommendation = intelligent_selector.get_business_recommendation(context)
        print(recommendation)
    
    # 6. æœ€ç»ˆç»“è®º
    print("\nğŸ† æœ€ç»ˆç»“è®ºå’Œå»ºè®®:")
    print("="*70)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = max(available_models, 
                    key=lambda x: comparator.performance_metrics[x].f1_score)
    best_metrics = comparator.performance_metrics[best_model]
    
    print(f"æ¨èæ¨¡å‹: {best_model}")
    print(f"æ€§èƒ½æŒ‡æ ‡: F1={best_metrics.f1_score:.4f}, å‡†ç¡®ç‡={best_metrics.accuracy:.4f}")
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§æ€»ç»“
    significant_tests = [r for r in ab_results if r.is_significant]
    print(f"ç»Ÿè®¡æ˜¾è‘—å·®å¼‚æµ‹è¯•: {len(significant_tests)}/{len(ab_results)} ç»„å¯¹æ¯”å…·æœ‰æ˜¾è‘—å·®å¼‚")
    
    # ç®—æ³•ç†è®ºåˆ†æ
    print("\nğŸ“š ç®—æ³•ç†è®ºåˆ†æ:")
    print("â€¢ XGBoost: äºŒé˜¶æ¢¯åº¦ä¼˜åŒ–ï¼Œå¼ºæ­£åˆ™åŒ–ï¼Œé€‚åˆç»“æ„åŒ–æ•°æ®")
    print("â€¢ LightGBM: åŸºäºç›´æ–¹å›¾ï¼Œå¶å­ç”Ÿé•¿ç­–ç•¥ï¼Œé€Ÿåº¦å¿«å†…å­˜çœ") 
    print("â€¢ RandomForest: é›†æˆå­¦ä¹ ï¼Œæ–¹å·®å‡å°‘ï¼Œå¯è§£é‡Šæ€§å¼º")
    print("â€¢ å»ºè®®: æ ¹æ®ä¸šåŠ¡åœºæ™¯é€‰æ‹©ï¼Œå¯è€ƒè™‘é›†æˆå¤šæ¨¡å‹")
    
    # è¿”å›ç»“æœå­—å…¸
    return {
        'best_model': best_model,
        'best_metrics': {
            'f1_score': best_metrics.f1_score,
            'accuracy': best_metrics.accuracy,
            'auc_score': best_metrics.auc_score,
            'training_time': best_metrics.training_time
        },
        'ab_test_results': [{
            'model_a': result.model_a_name,
            'model_b': result.model_b_name,
            'p_value': result.p_value,
            'is_significant': result.is_significant,
            'winner': result.winner
        } for result in ab_results],
        'data_shape': X.shape,
        'class_distribution': np.bincount(y).tolist(),
        'available_models': available_models
    }


if __name__ == "__main__":
    main()