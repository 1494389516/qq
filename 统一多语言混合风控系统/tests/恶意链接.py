import pandas as pd
import numpy as np
import tldextract
import re
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from collections import Counter
import time

# æ¨¡æ‹Ÿäº¤æ˜“æ•°æ®
data = {
    'user_id': [1, 1, 1, 1, 2],
    'timestamp': ['2025-09-25 12:00:00', '2025-09-25 12:05:00', '2025-09-25 12:10:00', '2025-09-25 12:15:00', '2025-09-25 12:00:00'],
    'url': ['http://example.com/offer?id=1', 
            'http://malicious.com/bad-link', 
            'http://example.com/offer?id=2', 
            'http://malicious.com/phishing', 
            'http://example.com/offer?id=3']
}

# å°†æ•°æ®è½¬æ¢ä¸º DataFrame
df = pd.DataFrame(data)

# è½¬æ¢ timestamp ä¸º datetime ç±»å‹
df['timestamp'] = pd.to_datetime(df['timestamp'])

# ä¼˜åŒ–åçš„ URL ç‰¹å¾æå–å‡½æ•°
def extract_url_features(url):
    from urllib.parse import urlparse
    
    # æå–åŸŸåå’Œè·¯å¾„
    try:
        ext = tldextract.extract(url)
        parsed_url = urlparse(url)
    except Exception as e:
        print(f"URLè§£æé”™è¯¯: {url}, é”™è¯¯: {e}")
        return get_default_features()
    
    domain = ext.domain + '.' + ext.suffix if ext.suffix else ext.domain
    path = parsed_url.path
    query = parsed_url.query
    
    # åŸºç¡€URLç‰¹å¾
    url_length = len(url)
    path_length = len(path)
    query_length = len(query)
    
    # URLç»“æ„åˆ†æç‰¹å¾
    is_short_url = 1 if url_length < 30 else 0
    is_long_url = 1 if url_length > 100 else 0
    has_query_param = 1 if query else 0
    has_subdomain = 1 if ext.subdomain else 0
    has_https = 1 if url.startswith('https') else 0
    has_ip_address = 1 if re.search(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', url) else 0
    
    # å¯ç–‘å­—ç¬¦ç»Ÿè®¡
    special_char_count = len(re.findall(r'[\-\_\%\@\&\=\?]', url))
    digit_count = len(re.findall(r'\d', url))
    
    # æ¶æ„å…³é”®è¯æ£€æµ‹ï¼ˆæ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼‰
    suspicious_keywords = [
        'offer', 'bad', 'phishing', 'free', 'click', 'win', 'secure',
        'verify', 'update', 'suspend', 'confirm', 'urgent', 'prize',
        'bonus', 'reward', 'download', 'install', 'activate'
    ]
    keyword_score = sum([1 for word in suspicious_keywords if re.search(rf'\b{word}\b', url.lower())])
    
    # åŸŸåå¯ç–‘æ€§æ£€æµ‹
    domain_suspicious = 0
    if domain:
        # æ£€æŸ¥åŸŸåæ˜¯å¦åŒ…å«æ•°å­—
        if re.search(r'\d', domain):
            domain_suspicious += 1
        # æ£€æŸ¥åŸŸåé•¿åº¦
        if len(domain) > 20:
            domain_suspicious += 1
        # æ£€æŸ¥æ˜¯å¦ä¸ºå·²çŸ¥å¯ç–‘åŸŸå
        suspicious_domains = ['malicious.com', 'phishing.net', 'scam.org']
        if domain.lower() in suspicious_domains:
            domain_suspicious += 3
    
    return {
        'domain': domain,
        'url_length': url_length,
        'path_length': path_length,
        'query_length': query_length,
        'is_short_url': is_short_url,
        'is_long_url': is_long_url,
        'has_query_param': has_query_param,
        'has_subdomain': has_subdomain,
        'has_https': has_https,
        'has_ip_address': has_ip_address,
        'special_char_count': special_char_count,
        'digit_count': digit_count,
        'keyword_score': keyword_score,
        'domain_suspicious': domain_suspicious,
    }

def get_default_features():
    """è¿”å›é»˜è®¤ç‰¹å¾å€¼ï¼ˆç”¨äºURLè§£æå¤±è´¥çš„æƒ…å†µï¼‰"""
    return {
        'domain': 'unknown',
        'url_length': 0,
        'path_length': 0,
        'query_length': 0,
        'is_short_url': 0,
        'is_long_url': 0,
        'has_query_param': 0,
        'has_subdomain': 0,
        'has_https': 0,
        'has_ip_address': 0,
        'special_char_count': 0,
        'digit_count': 0,
        'keyword_score': 0,
        'domain_suspicious': 0,
    }

# æå–æ¯ä¸ªURLçš„ç‰¹å¾
features = df['url'].apply(extract_url_features)
features_df = pd.DataFrame(features.tolist())

# åˆå¹¶ç‰¹å¾å’ŒåŸæ•°æ®
df = pd.concat([df, features_df], axis=1)

# ä¼˜åŒ–åçš„å±é™©æ€§è¯„åˆ†ç®—æ³•
def calculate_risk_score(row):
    """åŸºäºå¤šç»´åº¦ç‰¹å¾è®¡ç®—URLå±é™©æ€§è¯„åˆ†"""
    score = 0
    
    # URLé•¿åº¦ç›¸å…³è¯„åˆ†
    score += row['url_length'] * 0.01  # é•¿URLå¯èƒ½æ›´å¯ç–‘
    score += row['path_length'] * 0.1
    score += row['query_length'] * 0.15
    
    # ç»“æ„ç‰¹å¾è¯„åˆ†
    score += row['is_short_url'] * 2.0    # çŸ­é“¾æ¥é£é™©è¾ƒé«˜
    score += row['is_long_url'] * 1.0     # è¿‡é•¿URLä¹Ÿå¯ç–‘
    score += row['has_ip_address'] * 3.0  # ç›´æ¥ä½¿ç”¨IPåœ°å€å¾ˆå¯ç–‘
    score += row['special_char_count'] * 0.2  # ç‰¹æ®Šå­—ç¬¦è¿‡å¤šå¯ç–‘
    score += row['digit_count'] * 0.1     # æ•°å­—è¿‡å¤šå¯ç–‘
    
    # å†…å®¹ç‰¹å¾è¯„åˆ†
    score += row['keyword_score'] * 2.5   # æ¶æ„å…³é”®è¯æƒé‡æé«˜
    score += row['domain_suspicious'] * 2.0  # å¯ç–‘åŸŸåè¯„åˆ†
    
    # å®‰å…¨ç‰¹å¾å‡åˆ†
    score -= row['has_https'] * 1.5       # HTTPSç›¸å¯¹å®‰å…¨
    
    # ç¡®ä¿è¯„åˆ†ä¸ºæ­£æ•°
    return max(0, score)

# è®¡ç®—å±é™©æ€§åˆ†æ•°
df['risk_score'] = df.apply(calculate_risk_score, axis=1)

# ä½¿ç”¨æ”¹è¿›çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹
def detect_anomalies(df):
    """ä½¿ç”¨å¤šç§ç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹"""
    # é€‰æ‹©æ›´å¤šç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    feature_columns = [
        'url_length', 'path_length', 'query_length', 'keyword_score', 
        'is_short_url', 'is_long_url', 'has_ip_address', 
        'special_char_count', 'domain_suspicious'
    ]
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(df[feature_columns])
    
    # ä½¿ç”¨Isolation Forestè¿›è¡Œå¼‚å¸¸æ£€æµ‹
    model = IsolationForest(
        contamination='auto',   # è‡ªåŠ¨ç¡®å®šæ±¡æŸ“ç‡
        random_state=42,        # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
        n_estimators=200        # å¢åŠ æ ‘çš„æ•°é‡æé«˜å‡†ç¡®æ€§
    )
    
    # å…ˆè®­ç»ƒæ¨¡å‹ï¼Œå†è·å–å¼‚å¸¸æ ‡ç­¾å’Œè¯„åˆ†
    anomaly_labels = model.fit_predict(scaled_features)
    anomaly_scores = model.decision_function(scaled_features)
    
    return anomaly_labels, anomaly_scores

# æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
df['anomaly'], df['anomaly_score'] = detect_anomalies(df)

# ä¼˜åŒ–åçš„ç”¨æˆ·è¡Œä¸ºåˆ†æ
def analyze_user_behavior(df, time_window_minutes=10, suspicious_threshold=2):
    """åˆ†æç”¨æˆ·ç‚¹å‡»è¡Œä¸ºæ¨¡å¼"""
    time_window = pd.Timedelta(f'{time_window_minutes} min')
    df['time_diff'] = df.groupby('user_id')['timestamp'].diff().fillna(pd.Timedelta(seconds=0))
    df['frequent_malicious_click'] = 0
    df['click_velocity'] = 0.0  # ç‚¹å‡»é€Ÿåº¦
    
    for user_id in df['user_id'].unique():
        user_data = df[df['user_id'] == user_id].sort_values('timestamp')
        
        for i, row in user_data.iterrows():
            current_time = row['timestamp']
            time_limit = current_time - time_window
            
            # è®¡ç®—æ—¶é—´çª—å£å†…çš„ç‚¹å‡»ç»Ÿè®¡
            window_data = user_data[
                (user_data['timestamp'] >= time_limit) & 
                (user_data['timestamp'] <= current_time)
            ]
            
            # æ¶æ„é“¾æ¥é¢‘ç¹ç‚¹å‡»æ£€æµ‹
            malicious_clicks_in_window = window_data[window_data['anomaly'] == -1]
            if len(malicious_clicks_in_window) > suspicious_threshold:
                df.at[i, 'frequent_malicious_click'] = 1
            
            # è®¡ç®—ç‚¹å‡»é€Ÿåº¦ï¼ˆæ¯åˆ†é’Ÿç‚¹å‡»æ¬¡æ•°ï¼‰
            if len(window_data) > 1:
                click_velocity = len(window_data) / time_window_minutes
                df.at[i, 'click_velocity'] = click_velocity
    
    return df

# æ‰§è¡Œç”¨æˆ·è¡Œä¸ºåˆ†æ
df = analyze_user_behavior(df)

# ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
def generate_analysis_report(df):
    """ç”Ÿæˆæ¶æ„é“¾æ¥æ£€æµ‹åˆ†ææŠ¥å‘Š"""
    print("=" * 80)
    print("æ¶æ„é“¾æ¥æ£€æµ‹åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    total_urls = len(df)
    malicious_urls = len(df[df['anomaly'] == -1])
    malicious_rate = (malicious_urls / total_urls) * 100 if total_urls > 0 else 0
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   - æ€»URLæ•°é‡: {total_urls}")
    print(f"   - æ£€æµ‹åˆ°æ¶æ„URL: {malicious_urls}")
    print(f"   - æ¶æ„URLæ¯”ä¾‹: {malicious_rate:.2f}%")
    
    # ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡
    print(f"\nğŸ‘¤ ç”¨æˆ·è¡Œä¸ºåˆ†æ:")
    for user_id in df['user_id'].unique():
        user_data = df[df['user_id'] == user_id]
        user_malicious = len(user_data[user_data['anomaly'] == -1])
        user_frequent = len(user_data[user_data['frequent_malicious_click'] == 1])
        avg_risk_score = user_data['risk_score'].mean()
        print(f"   - ç”¨æˆ· {user_id}: æ¶æ„é“¾æ¥ {user_malicious}ä¸ª, é¢‘ç¹ç‚¹å‡» {user_frequent}æ¬¡, å¹³å‡é£é™©è¯„åˆ† {avg_risk_score:.2f}")
    
    # è¯¦ç»†ç»“æœ
    print(f"\nğŸ“‹ è¯¦ç»†æ£€æµ‹ç»“æœ:")
    result_columns = [
        'user_id', 'timestamp', 'url', 'risk_score', 
        'anomaly', 'anomaly_score', 'frequent_malicious_click', 'click_velocity'
    ]
    
    for idx, row in df.iterrows():
        status = "ğŸš¨ æ¶æ„" if row['anomaly'] == -1 else "âœ… æ­£å¸¸"
        print(f"\n   {status} | ç”¨æˆ·{row['user_id']} | {row['timestamp']}")
        print(f"      URL: {row['url']}")
        print(f"      é£é™©è¯„åˆ†: {row['risk_score']:.2f} | å¼‚å¸¸è¯„åˆ†: {row['anomaly_score']:.3f}")
        if row['frequent_malicious_click'] == 1:
            print(f"      âš ï¸  æ£€æµ‹åˆ°é¢‘ç¹æ¶æ„ç‚¹å‡»è¡Œä¸º")
        if row['click_velocity'] > 0:
            print(f"      ç‚¹å‡»é€Ÿåº¦: {row['click_velocity']:.2f} æ¬¡/åˆ†é’Ÿ")
    
    print("\n" + "=" * 80)
    return df[result_columns]

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
result_df = generate_analysis_report(df)

